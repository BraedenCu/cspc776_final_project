{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Minimal DQN for Milling Environment with Reward Normalization, \n",
    "Enhanced TensorBoard Logging, and GPU Support\n",
    "\n",
    "This script implements a minimal DQN applied to our 3D milling environment \n",
    "(Milling3DEnvNoOverhang). The environment simulates a voxel grid where:\n",
    "  - Stock cells (value 0) are meant to be removed.\n",
    "  - Forbidden cells (value 2) represent the target region and boundaries.\n",
    "  - The router starts at a safe location.\n",
    "  \n",
    "The DQN learns to select a target coordinate (discrete action) for the router \n",
    "to move to (removing stock along the path), with rewards given exclusively for \n",
    "removing non-target stock. Episodes run for a fixed number of steps (max_steps) \n",
    "unless a violation occurs. At the end of an episode (if no violation), a bonus is \n",
    "given based on the fraction of stock cleared.\n",
    "\n",
    "The rewards are normalized using an exponential moving average to reduce variance.\n",
    "Training metrics (loss, episode reward, steps, stock removed, fraction removed, termination reasons) \n",
    "are logged to TensorBoard.\n",
    "\n",
    "Usage:\n",
    "  python dqn_tensorboard_gpu.py\n",
    "Then launch TensorBoard with:\n",
    "  tensorboard --logdir=runs\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Environment: MillingEnvironment\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "class MillingEnvironment:\n",
    "    def __init__(self, grid_size=8, min_radius=2, max_radius=3, max_steps=50):\n",
    "        self.grid_size = grid_size\n",
    "        self.min_radius = min_radius\n",
    "        self.max_radius = max_radius\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        n = self.grid_size\n",
    "        # Create stock: all voxels initially present (value 0 means stock to remove)\n",
    "        self.stock = np.ones((n, n, n), dtype=bool)\n",
    "        # Create shape: a half-sphere (target region) anchored at z=0, marked as forbidden.\n",
    "        self.shape = np.zeros((n, n, n), dtype=bool)\n",
    "        r = np.random.randint(self.min_radius, self.max_radius + 1)\n",
    "        # Choose random center in x,y ensuring shape fits; z is anchored at 0.\n",
    "        cx = np.random.randint(r, n - r)\n",
    "        cy = np.random.randint(r, n - r)\n",
    "        cz = 0\n",
    "        for x in range(n):\n",
    "            for y in range(n):\n",
    "                for z in range(n):\n",
    "                    if z >= 0:\n",
    "                        dx = x - cx\n",
    "                        dy = y - cy\n",
    "                        dz = z - cz\n",
    "                        if dx*dx + dy*dy + dz*dz <= r*r:\n",
    "                            self.shape[x, y, z] = True\n",
    "\n",
    "        # Router starts at a safe corner.\n",
    "        self.router_pos = np.array([1, 1, 1], dtype=np.int32)\n",
    "        self.steps_taken = 0\n",
    "        self.done = False\n",
    "        self.total_removed = 0.0\n",
    "        self.termination_reason = None\n",
    "        return self._get_observation()\n",
    "\n",
    "    def line_voxels(self, start, end):\n",
    "        s = start.astype(float)\n",
    "        e = end.astype(float)\n",
    "        diff = e - s\n",
    "        length = int(np.linalg.norm(diff))\n",
    "        if length == 0:\n",
    "            return [tuple(start)]\n",
    "        steps = max(1, length * 2)\n",
    "        visited = set()\n",
    "        out = []\n",
    "        for i in range(steps + 1):\n",
    "            t = i / steps\n",
    "            coords = np.round(s + diff * t).astype(int)\n",
    "            c_tuple = tuple(coords)\n",
    "            if c_tuple not in visited:\n",
    "                visited.add(c_tuple)\n",
    "                out.append(c_tuple)\n",
    "        return out\n",
    "\n",
    "    def do_move(self, target):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, True\n",
    "\n",
    "        path_vox = self.line_voxels(self.router_pos, target)\n",
    "        failed = False\n",
    "        reward = 0.0\n",
    "        reason = None\n",
    "\n",
    "        for vx, vy, vz in path_vox:\n",
    "            if not (0 <= vx < self.grid_size and 0 <= vy < self.grid_size and 0 <= vz < self.grid_size):\n",
    "                failed = True\n",
    "                reward -= 20.0  # moderate penalty for out-of-bounds\n",
    "                reason = \"out_of_bounds\"\n",
    "                break\n",
    "            if self.shape[vx, vy, vz]:\n",
    "                failed = True\n",
    "                reward -= 20.0  # moderate penalty for cutting target\n",
    "                reason = \"shape\"\n",
    "                break\n",
    "            if self.stock[vx, vy, vz]:\n",
    "                self.stock[vx, vy, vz] = False\n",
    "                reward += 1.0  # reward for removing stock\n",
    "                self.total_removed += 1.0\n",
    "\n",
    "        if not failed:\n",
    "            reward -= 0.1  # small step cost\n",
    "            self.router_pos = target.copy()\n",
    "\n",
    "        self.steps_taken += 1\n",
    "        if self.steps_taken >= self.max_steps and not failed:\n",
    "            failed = True\n",
    "            reason = \"max_steps\"\n",
    "\n",
    "        outside_mask = (self.shape == 0)\n",
    "        if not failed and np.sum(self.stock[outside_mask]) == 0:\n",
    "            reward += 50.0  # bonus for complete removal\n",
    "            failed = True\n",
    "            reason = \"complete\"\n",
    "\n",
    "        if failed:\n",
    "            self.done = True\n",
    "            self.termination_reason = reason\n",
    "\n",
    "        return self._get_observation(), reward, self.done\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Flatten stock (bool->float) and shape (bool->float) and append router coordinates.\n",
    "        stock_f = self.stock.flatten().astype(np.float32)\n",
    "        shape_f = self.shape.flatten().astype(np.float32)\n",
    "        router_f = self.router_pos.astype(np.float32)\n",
    "        return np.concatenate([stock_f, shape_f, router_f], axis=0)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Decode action (discrete in [0, grid_size^3 -1]) into (x,y,z)\n",
    "        n = self.grid_size\n",
    "        z = action % n\n",
    "        y = (action // n) % n\n",
    "        x = (action // (n * n)) % n\n",
    "        next_obs, reward, done = self.do_move(np.array([x, y, z], dtype=np.int32))\n",
    "        return next_obs, reward, done, {}\n",
    "\n",
    "    def reset_gym(self):\n",
    "        return self.reset()\n",
    "\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return self.grid_size**3\n",
    "\n",
    "    def fraction_outside_removed(self):\n",
    "        outside_mask = (self.shape == 0)\n",
    "        total_outside = np.sum(outside_mask)\n",
    "        return self.total_removed / (total_outside + 1e-8)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) Reward Normalizer\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "class RewardNormalizer:\n",
    "    def __init__(self, momentum=0.99, eps=1e-8):\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "\n",
    "    def update(self, r):\n",
    "        self.mean = self.momentum * self.mean + (1 - self.momentum) * r\n",
    "        self.var = self.momentum * self.var + (1 - self.momentum) * ((r - self.mean) ** 2)\n",
    "\n",
    "    def normalize(self, r):\n",
    "        return (r - self.mean) / (np.sqrt(self.var) + self.eps)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) Simple DQN and Replay Buffer\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "class SimpleDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super(SimpleDQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, s, a, r, s_next, done):\n",
    "        self.buffer.append((s, a, r, s_next, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s_next, done = zip(*batch)\n",
    "        return np.array(s), np.array(a), np.array(r), np.array(s_next), np.array(done)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) DQN Training Loop with TensorBoard Logging and Reward Normalization\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def train_dqn(env, num_episodes=200, batch_size=32, gamma=0.99, lr=1e-5,\n",
    "              epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.99995, replay_capacity=10000):\n",
    "    # Use GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    obs_dim = len(env.reset_gym())\n",
    "    n_actions = env.n_actions\n",
    "\n",
    "    dqn = SimpleDQN(obs_dim, n_actions).to(device)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
    "    replay = ReplayBuffer(capacity=replay_capacity)\n",
    "\n",
    "    reward_normalizer = RewardNormalizer(momentum=0.99)\n",
    "\n",
    "    log_dir = os.path.join(\"runs\", f\"dqn_milling_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    global_step = 0\n",
    "    update_count = 0\n",
    "\n",
    "    term_counters = {\"shape\": 0, \"out_of_bounds\": 0, \"max_steps\": 0, \"complete\": 0}\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset_gym()\n",
    "        obs_t = torch.FloatTensor(obs).to(device)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps_this_ep = 0\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, n_actions - 1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = dqn(obs_t.unsqueeze(0))\n",
    "                    action = q_vals.argmax(dim=1).item()\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            reward_normalizer.update(reward)\n",
    "            norm_reward = reward_normalizer.normalize(reward)\n",
    "            total_reward += reward\n",
    "\n",
    "            replay.push(obs, action, norm_reward, next_obs, done)\n",
    "\n",
    "            obs = next_obs\n",
    "            obs_t = torch.FloatTensor(obs).to(device)\n",
    "            steps_this_ep += 1\n",
    "\n",
    "            if len(replay) >= batch_size:\n",
    "                s_arr, a_arr, r_arr, s_next_arr, d_arr = replay.sample(batch_size)\n",
    "                s_ten = torch.FloatTensor(s_arr).to(device)\n",
    "                a_ten = torch.LongTensor(a_arr).to(device)\n",
    "                r_ten = torch.FloatTensor(r_arr).to(device)\n",
    "                s_next_ten = torch.FloatTensor(s_next_arr).to(device)\n",
    "                d_ten = torch.BoolTensor(d_arr).to(device)\n",
    "\n",
    "                q_vals = dqn(s_ten)\n",
    "                q_s_a = q_vals.gather(1, a_ten.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q_next = dqn(s_next_ten)\n",
    "                    max_q_next, _ = torch.max(q_next, dim=1)\n",
    "                    max_q_next[d_ten] = 0.0\n",
    "                target = r_ten + gamma * max_q_next\n",
    "\n",
    "                loss = nn.MSELoss()(q_s_a, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Optional: Gradient clipping can be applied here if necessary:\n",
    "                # torch.nn.utils.clip_grad_norm_(dqn.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                update_count += 1\n",
    "                writer.add_scalar(\"Loss\", loss.item(), update_count)\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        term_reason = env.termination_reason if env.termination_reason is not None else \"max_steps\"\n",
    "        if term_reason in term_counters:\n",
    "            term_counters[term_reason] += 1\n",
    "\n",
    "        writer.add_scalar(\"EpisodeReward\", total_reward, ep)\n",
    "        writer.add_scalar(\"StepsPerEpisode\", steps_this_ep, ep)\n",
    "        writer.add_scalar(\"StockRemoved\", env.total_removed, ep)\n",
    "        outside_mask = (env.shape == 0)\n",
    "        outside_total = np.sum(outside_mask)\n",
    "        frac_removed = env.total_removed / (outside_total + 1e-8)\n",
    "        writer.add_scalar(\"FractionRemoved\", frac_removed, ep)\n",
    "        writer.add_scalar(\"WorkerMoves\", env.steps_taken, ep)\n",
    "        writer.add_scalar(\"Epsilon\", epsilon, ep)\n",
    "\n",
    "        print(f\"Episode {ep+1}/{num_episodes}: Reward={total_reward:.2f}, Steps={steps_this_ep}, \"\n",
    "              f\"Epsilon={epsilon:.2f}, StockRemoved={env.total_removed:.0f}, \"\n",
    "              f\"FractionRemoved={frac_removed:.3f}, Termination={term_reason}\")\n",
    "\n",
    "    for reason, count in term_counters.items():\n",
    "        writer.add_scalar(f\"Terminations/{reason}\", count, 0)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training complete.\")\n",
    "    return dqn\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) Testing Routine: Evaluate over 100 Episodes\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def test_dqn(dqn, env, num_tests=100):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    total_frac = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        obs = env.reset_gym()\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.FloatTensor(obs).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_vals = dqn(obs_t.unsqueeze(0))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "        frac_removed = env.fraction_outside_removed()  # fraction from 0 to 1\n",
    "        total_frac += frac_removed\n",
    "        total_steps += steps\n",
    "\n",
    "    avg_frac = total_frac / num_tests\n",
    "    avg_steps = total_steps / num_tests\n",
    "    print(f\"Average fraction of outside stock removed: {avg_frac*100:.2f}%\")\n",
    "    print(f\"Average number of steps per episode: {avg_steps:.2f}\")\n",
    "    return avg_frac, avg_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 6) Main Script\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    env = MillingEnvironment(grid_size=8, min_radius=1, max_radius=3, max_steps=30)\n",
    "    trained_dqn = train_dqn(env, num_episodes=30000)\n",
    "    print(\"Training finished. Running tests...\")\n",
    "    test_dqn(trained_dqn, env, num_tests=100)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# now run tensorboard\n",
    "!tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
