{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURR V2 V2\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# CURRENT\n",
    "\"\"\"\n",
    "Fully‑GPU Batched Sculpt3DEnv + DQN Agent (Hybrid State & Noisy Nets)\n",
    "\n",
    "\n",
    "• State Representation: Hybrid - CNN processes Stock/Mask grids, concatenates\n",
    "                      with normalized XYZ coordinates.\n",
    "• Agent Network: Uses a 3D CNN feature extractor + Dense head with Noisy Layers.\n",
    "• Exploration: Uses Noisy Networks instead of epsilon-greedy.\n",
    "• N envs stepped in parallel during training.\n",
    "• Batched replay buffer insertion.\n",
    "• Includes periodic evaluation during training and final evaluation/rendering.\n",
    "• Plots carving performance trend at the end of training.\n",
    "• **NEW:** Periodic saving of model weights during training.\n",
    "\n",
    "\n",
    "NOTE: Monitor memory usage. Tune hyperparameters (LR, network, etc.).\n",
    "     Periodic evaluation adds runtime overhead.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import time\n",
    "import math # For noisy layer initialization\n",
    "import io   # For TensorBoard image logging\n",
    "\n",
    "\n",
    "# --- Configure GPU Memory Growth ---\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    " try:\n",
    "   for gpu in gpus:\n",
    "     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "   logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "   print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs configured with Memory Growth.\")\n",
    " except RuntimeError as e:\n",
    "   print(f\"Memory growth error: {e}\") # Might happen if GPU already initialized\n",
    "else:\n",
    "   print(\"No GPU detected by TensorFlow.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Batched Sculpt3DEnvTF (Hybrid Observation)\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedSculpt3DEnvTF:\n",
    "   def __init__(self, grid_size=16, max_steps=200, n_envs=16):\n",
    "       G, N = grid_size, n_envs\n",
    "       if N <= 0: raise ValueError(\"n_envs must be positive.\")\n",
    "       self.G, self.N, self.max_steps = G, N, max_steps\n",
    "       self.flat_dim = G*G*G\n",
    "       self.grid_obs_shape = (G, G, G, 2) # Channels: Stock, ShapeMask\n",
    "       self.coord_obs_shape = (3,)        # Channels: X, Y, Z (normalized)\n",
    "\n",
    "\n",
    "       # Use tf operations for shape generation to keep it within TF graph if possible\n",
    "       coords_range = tf.range(G, dtype=tf.float32)\n",
    "       coords = tf.stack(tf.meshgrid(coords_range, coords_range, coords_range, indexing='ij'), axis=-1) # [G,G,G,3]\n",
    "       center = tf.constant([G/2 - 0.5, G/2 - 0.5, G/2 - 0.5], tf.float32) # Center point\n",
    "       dist2 = tf.reduce_sum(tf.square(coords - center), axis=-1) # [G,G,G] squared distance from center\n",
    "       radius_sq = tf.square(tf.cast(G // 2 - 1, tf.float32)) # Define radius (slightly smaller than half grid)\n",
    "       mask3d = dist2 <= radius_sq # Boolean mask [G,G,G]\n",
    "       mask_flat = tf.reshape(mask3d, [-1]) # [G^3]\n",
    "\n",
    "\n",
    "       # Initialize state variables\n",
    "       self.shape_mask = tf.Variable(tf.tile(mask_flat[None, :], [N, 1]), trainable=False, dtype=tf.bool, name=\"shape_mask\")\n",
    "       self.stock = tf.Variable(tf.ones([N, self.flat_dim], dtype=tf.bool), trainable=False, name=\"stock\") # Start with full stock\n",
    "       self.pos = tf.Variable(tf.zeros([N], dtype=tf.int32), trainable=False, name=\"pos\")\n",
    "       self.steps = tf.Variable(tf.zeros([N], dtype=tf.int32), trainable=False, name=\"steps\")\n",
    "       self.done = tf.Variable(tf.zeros([N], dtype=tf.bool), trainable=False, name=\"done\")\n",
    "\n",
    "\n",
    "       # Calculate shifts (outside tf.function, Python is fine)\n",
    "       G_py = grid_size # Use Python int for calculation\n",
    "       def to_flat_py(dx, dy, dz): return dx*G_py*G_py + dy*G_py + dz\n",
    "       moves = [(1,0,0),(-1,0,0),(0,1,0),(0,-1,0),(0,0,1),(0,0,-1)] # dx, dy, dz\n",
    "       shifts_py = [to_flat_py(*m) for m in moves]\n",
    "       self.shifts = tf.constant(shifts_py, dtype=tf.int32, name=\"shifts\")\n",
    "\n",
    "\n",
    "   @tf.function\n",
    "   def reset(self):\n",
    "       # Reset stock to full, steps/done to zero\n",
    "       self.stock.assign(tf.ones_like(self.stock))\n",
    "       self.steps.assign(tf.zeros_like(self.steps))\n",
    "       self.done.assign(tf.zeros_like(self.done))\n",
    "\n",
    "\n",
    "       # Find safe starting positions (outside the target shape)\n",
    "       # Use shape_mask[0] as reference (they are identical initially)\n",
    "       safe_indices = tf.where(tf.logical_not(self.shape_mask[0]))[:, 0] # Get flat indices\n",
    "       num_safe = tf.shape(safe_indices)[0]\n",
    "       tf.Assert(num_safe >= self.N, [\"Not enough safe starting positions available.\", num_safe, self.N])\n",
    "\n",
    "\n",
    "       # Shuffle and select N starting positions\n",
    "       shuffled_safe_indices = tf.random.shuffle(safe_indices)[:self.N]\n",
    "       self.pos.assign(tf.cast(shuffled_safe_indices, tf.int32))\n",
    "\n",
    "\n",
    "       return self._get_obs()\n",
    "\n",
    "\n",
    "   @tf.function\n",
    "   def step(self, actions):\n",
    "       # Calculate potential new positions based on actions\n",
    "       action_shifts = tf.gather(self.shifts, actions) # [N] shifts based on actions\n",
    "       new_pos = self.pos + action_shifts             # [N] potential new flat indices\n",
    "\n",
    "\n",
    "       # Check boundaries and collisions\n",
    "       in_bounds = tf.logical_and(new_pos >= 0, new_pos < self.flat_dim) # [N] boolean\n",
    "       # Clip new_pos for safe gathering, even if OOB\n",
    "       safe_new_pos = tf.clip_by_value(new_pos, 0, self.flat_dim - 1)\n",
    "\n",
    "\n",
    "       # Gather shape mask and stock at the potential new positions\n",
    "       shape_mask_at_new = tf.gather(self.shape_mask, safe_new_pos, axis=1, batch_dims=1) # [N] bool\n",
    "       stock_at_new = tf.gather(self.stock, safe_new_pos, axis=1, batch_dims=1)      # [N] bool\n",
    "\n",
    "\n",
    "       # Determine invalid moves (hit shape or out of bounds)\n",
    "       hit_shape_or_oob = tf.logical_or(tf.logical_not(in_bounds), shape_mask_at_new) # [N] bool\n",
    "\n",
    "\n",
    "       # Determine if stock can be removed (valid move AND stock exists at new pos)\n",
    "       can_remove = tf.logical_and(tf.logical_not(hit_shape_or_oob), stock_at_new) # [N] bool\n",
    "\n",
    "\n",
    "       # Calculate rewards\n",
    "       reward = tf.where(hit_shape_or_oob, -5.0, 0.0)   # Penalty for invalid move\n",
    "       reward = tf.where(can_remove, reward + 1.0, reward) # Reward for removing stock\n",
    "       reward = reward - 0.1                             # Step penalty\n",
    "\n",
    "\n",
    "       # Update stock (remove material where applicable)\n",
    "       remove_indices = tf.where(can_remove) # Indices [k, 0] where can_remove is True\n",
    "       num_removals = tf.shape(remove_indices)[0]\n",
    "\n",
    "\n",
    "       # Conditional update to avoid empty tensor issues if num_removals is 0\n",
    "       def perform_update():\n",
    "           # Indices of environments where removal happened\n",
    "           env_indices_to_update = tf.squeeze(tf.cast(remove_indices, tf.int32), axis=1) # [num_removals]\n",
    "           # Corresponding positions where removal happened\n",
    "           pos_to_remove = tf.gather(new_pos, env_indices_to_update) # [num_removals]\n",
    "           # Combine env and pos indices for scatter_nd_update: [[env0, pos0], [env1, pos1], ...]\n",
    "           scatter_indices = tf.stack([env_indices_to_update, pos_to_remove], axis=1) # [num_removals, 2]\n",
    "           # Values to update with (False, meaning remove stock)\n",
    "           updates = tf.zeros(num_removals, dtype=tf.bool)\n",
    "           return tf.tensor_scatter_nd_update(self.stock, scatter_indices, updates)\n",
    "\n",
    "\n",
    "       maybe_updated_stock = tf.cond(num_removals > 0,\n",
    "                                     true_fn=perform_update,\n",
    "                                     false_fn=lambda: self.stock) # No update if no removals\n",
    "       self.stock.assign(maybe_updated_stock)\n",
    "\n",
    "\n",
    "       # Update agent position (only if the move was valid)\n",
    "       is_valid_move = tf.logical_not(hit_shape_or_oob) # [N] bool\n",
    "       next_pos = tf.where(is_valid_move, new_pos, self.pos) # Stay if invalid, move if valid\n",
    "       self.pos.assign(next_pos)\n",
    "\n",
    "\n",
    "       # Update steps and check for done state\n",
    "       self.steps.assign_add(tf.ones_like(self.steps))\n",
    "       newly_done = (self.steps >= self.max_steps)\n",
    "       self.done.assign(tf.logical_or(self.done, newly_done)) # Mark as done if max steps reached\n",
    "\n",
    "\n",
    "       # Get next observation\n",
    "       next_obs = self._get_obs()\n",
    "       return next_obs, tf.cast(reward, tf.float32), self.done\n",
    "\n",
    "\n",
    "   @tf.function\n",
    "   def _get_obs(self):\n",
    "       G = self.G; N = self.N\n",
    "       # Reshape flat stock/shape masks to 3D grids\n",
    "       stock_grid = tf.reshape(self.stock, [N, G, G, G])\n",
    "       shape_mask_grid = tf.reshape(self.shape_mask, [N, G, G, G])\n",
    "\n",
    "\n",
    "       # Convert to float and stack as channels\n",
    "       stock_float = tf.cast(stock_grid, tf.float32)\n",
    "       shape_mask_float = tf.cast(shape_mask_grid, tf.float32)\n",
    "       grid_obs = tf.stack([stock_float, shape_mask_float], axis=-1) # [N, G, G, G, 2]\n",
    "\n",
    "\n",
    "       # Calculate normalized coordinates\n",
    "       g_tf = tf.constant(G, dtype=tf.int32)\n",
    "       z = self.pos % g_tf\n",
    "       y = (self.pos // g_tf) % g_tf\n",
    "       x = self.pos // (g_tf * g_tf)\n",
    "\n",
    "\n",
    "       # Normalize coordinates to [0, 1] range\n",
    "       g_minus_1_float = tf.cast(tf.maximum(1, G - 1), tf.float32) # Avoid division by zero if G=1\n",
    "       x_norm = tf.cast(x, tf.float32) / g_minus_1_float\n",
    "       y_norm = tf.cast(y, tf.float32) / g_minus_1_float\n",
    "       z_norm = tf.cast(z, tf.float32) / g_minus_1_float\n",
    "\n",
    "\n",
    "       coord_obs = tf.stack([x_norm, y_norm, z_norm], axis=-1) # [N, 3]\n",
    "\n",
    "\n",
    "       return (grid_obs, coord_obs) # Return as a tuple\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Replay Buffer (Handling Tuple State)\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedReplayBuffer:\n",
    "   def __init__(self, capacity=50000):\n",
    "       self.cap = capacity\n",
    "       # Use a deque for efficient pop from left when capacity is reached\n",
    "       self.buf = deque(maxlen=capacity)\n",
    "\n",
    "\n",
    "   def add_batch(self, S_tuple, A, R, S2_tuple, D):\n",
    "       # Store the tuple state directly along with A, R, D\n",
    "       self.buf.append((S_tuple, A, R, S2_tuple, D))\n",
    "\n",
    "\n",
    "   def sample(self, batch_size=32):\n",
    "       \"\"\"Samples a batch of transitions, handling the tuple state.\"\"\"\n",
    "       num_stored = len(self.buf)\n",
    "       if num_stored < batch_size:\n",
    "           # Not enough samples yet\n",
    "           return None\n",
    "\n",
    "\n",
    "       # Randomly sample indices\n",
    "       indices = random.sample(range(num_stored), batch_size)\n",
    "       # Retrieve the selected transitions (which are batches themselves)\n",
    "       batch = [self.buf[i] for i in indices]\n",
    "\n",
    "\n",
    "       # Unzip the components of the sampled transitions\n",
    "       S_tuple_list, A_list, R_list, S2_tuple_list, D_list = zip(*batch)\n",
    "\n",
    "\n",
    "       # Unzip the state tuples\n",
    "       S_grid_list, S_coord_list = zip(*S_tuple_list)\n",
    "       S2_grid_list, S2_coord_list = zip(*S2_tuple_list)\n",
    "\n",
    "\n",
    "       # Concatenate components into final batch tensors\n",
    "       # Axis 0 is the batch dimension to concatenate along\n",
    "       return (\n",
    "           tf.concat(S_grid_list, axis=0), tf.concat(S_coord_list, axis=0), # State S\n",
    "           tf.concat(A_list, axis=0),                                      # Action A\n",
    "           tf.concat(R_list, axis=0),                                      # Reward R\n",
    "           tf.concat(S2_grid_list, axis=0), tf.concat(S2_coord_list, axis=0),# Next state S2\n",
    "           tf.concat(D_list, axis=0)                                       # Done D\n",
    "       )\n",
    "\n",
    "\n",
    "   def __len__(self):\n",
    "       # Returns the number of *batches* stored, not total transitions\n",
    "       return len(self.buf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Noisy Dense Layer (Factorized Gaussian Noise)\n",
    "# -----------------------------------------------------------------------------\n",
    "class NoisyDense(tf.keras.layers.Layer):\n",
    "   def __init__(self, units, activation=None, sigma0=0.5, **kwargs):\n",
    "       super().__init__(**kwargs)\n",
    "       self.units = units\n",
    "       self.activation = tf.keras.activations.get(activation)\n",
    "       self.sigma0 = sigma0 # Initial standard deviation parameter\n",
    "\n",
    "\n",
    "   def build(self, input_shape):\n",
    "       in_features = input_shape[-1]\n",
    "       out_features = self.units\n",
    "       dtype = tf.float32 # Assuming float32\n",
    "\n",
    "\n",
    "       # Weight parameters (mean and standard deviation)\n",
    "       sigma_init_val = self.sigma0 / math.sqrt(float(in_features))\n",
    "       sigma_initializer = tf.constant_initializer(sigma_init_val)\n",
    "       self.kernel_mean = self.add_weight(name=\"kernel_mean\", shape=(in_features, out_features),\n",
    "                                          initializer=\"he_uniform\", trainable=True, dtype=dtype)\n",
    "       self.kernel_sigma = self.add_weight(name=\"kernel_sigma\", shape=(in_features, out_features),\n",
    "                                           initializer=sigma_initializer, trainable=True, dtype=dtype)\n",
    "\n",
    "\n",
    "       # Bias parameters (mean and standard deviation)\n",
    "       self.bias_mean = self.add_weight(name=\"bias_mean\", shape=(out_features,),\n",
    "                                        initializer=\"zeros\", trainable=True, dtype=dtype)\n",
    "       self.bias_sigma = self.add_weight(name=\"bias_sigma\", shape=(out_features,),\n",
    "                                         initializer=sigma_initializer, trainable=True, dtype=dtype)\n",
    "\n",
    "\n",
    "       super().build(input_shape)\n",
    "\n",
    "\n",
    "   def call(self, inputs, training=None):\n",
    "       # Determine if in training mode (for applying noise)\n",
    "       if training is None:\n",
    "           training = tf.keras.backend.learning_phase()\n",
    "\n",
    "\n",
    "       if training:\n",
    "           # Sample noise for weights and biases using Factorized Gaussian noise\n",
    "           # Generate noise for input and output dimensions\n",
    "           noise_in = self._factorized_noise(tf.shape(inputs)[-1])  # Shape [in_features]\n",
    "           noise_out = self._factorized_noise(self.units)           # Shape [out_features]\n",
    "\n",
    "\n",
    "           # Combine noise for weight matrix: outer product\n",
    "           # noise_in: [in_features], noise_out: [out_features]\n",
    "           # Need noise_in as [in_features, 1] and noise_out as [1, out_features] for matmul/tensordot\n",
    "           # Correct approach is outer product, can achieve with broadcasting or einsum\n",
    "           # Equivalent to: kernel_noise = noise_in[:, None] * noise_out[None, :]\n",
    "           kernel_noise = tf.tensordot(tf.expand_dims(noise_in, -1), tf.expand_dims(noise_out, 0), axes=1) # Shape [in_features, out_features]\n",
    "\n",
    "\n",
    "           # Noise for bias is just the output noise\n",
    "           bias_noise = noise_out # Shape [out_features]\n",
    "\n",
    "\n",
    "           # Apply noise: W = W_mu + W_sigma * noise_W, b = b_mu + b_sigma * noise_b\n",
    "           kernel = self.kernel_mean + self.kernel_sigma * kernel_noise\n",
    "           bias = self.bias_mean + self.bias_sigma * bias_noise\n",
    "       else:\n",
    "           # In inference mode, use only the mean weights and biases\n",
    "           kernel = self.kernel_mean\n",
    "           bias = self.bias_mean\n",
    "\n",
    "\n",
    "       # Standard dense layer calculation: output = input * kernel + bias\n",
    "       output = tf.matmul(inputs, kernel) + bias\n",
    "\n",
    "\n",
    "       # Apply activation function if specified\n",
    "       if self.activation is not None:\n",
    "           output = self.activation(output)\n",
    "       return output\n",
    "\n",
    "\n",
    "   def _factorized_noise(self, num_elements):\n",
    "       \"\"\"Generates noise based on the Factorized Gaussian noise formula.\"\"\"\n",
    "       # Sample standard normal noise\n",
    "       noise = tf.random.normal(shape=[num_elements])\n",
    "       # Apply transformation: sign(x) * sqrt(|x|)\n",
    "       return tf.sign(noise) * tf.sqrt(tf.abs(noise))\n",
    "\n",
    "\n",
    "   def compute_output_shape(self, input_shape):\n",
    "       # Output shape is same as input shape except for the last dimension (units)\n",
    "       return tuple(input_shape[:-1]) + (self.units,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Batched DQN Agent (Hybrid State CNN + Noisy Nets)\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedDQNAgentTF:\n",
    "   def __init__(self, grid_shape, coord_shape, action_dim=6,\n",
    "                lr=1e-4, gamma=0.99, tau=0.005):\n",
    "       self.grid_shape = grid_shape\n",
    "       self.coord_shape = coord_shape\n",
    "       self.action_dim = action_dim\n",
    "       self.gamma = gamma\n",
    "       self.tau = tau\n",
    "\n",
    "\n",
    "       # --- Build Model Function ---\n",
    "       def build_hybrid_noisy_model(name=\"Model\"):\n",
    "           grid_input = tf.keras.layers.Input(shape=self.grid_shape, name=\"grid_input\")\n",
    "           coord_input = tf.keras.layers.Input(shape=self.coord_shape, name=\"coord_input\")\n",
    "\n",
    "\n",
    "           # CNN part for grid observations\n",
    "           x_cnn = tf.keras.layers.Conv3D(filters=32, kernel_size=5, strides=2, activation='relu', padding='same')(grid_input)\n",
    "           x_cnn = tf.keras.layers.Conv3D(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')(x_cnn)\n",
    "           x_cnn = tf.keras.layers.Conv3D(filters=64, kernel_size=3, strides=1, activation='relu', padding='same')(x_cnn)\n",
    "           cnn_features = tf.keras.layers.Flatten()(x_cnn)\n",
    "\n",
    "\n",
    "           # Concatenate CNN features with coordinate observations\n",
    "           concat_features = tf.keras.layers.Concatenate()([cnn_features, coord_input])\n",
    "\n",
    "\n",
    "           # Dense part with Noisy Layers\n",
    "           x = NoisyDense(512, activation='relu')(concat_features)\n",
    "           outputs = NoisyDense(action_dim, activation='linear')(x) # Linear output for Q-values\n",
    "\n",
    "\n",
    "           return tf.keras.Model(inputs=[grid_input, coord_input], outputs=outputs, name=name)\n",
    "       # --- End Build Model Function ---\n",
    "\n",
    "\n",
    "       # Initialize online and target models\n",
    "       self.model = build_hybrid_noisy_model(name=\"Online_Model\")\n",
    "       self.target = build_hybrid_noisy_model(name=\"Target_Model\")\n",
    "       # Ensure target network starts with the same weights as the online network\n",
    "       self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "       # Optimizer\n",
    "       self.opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "       # Replay Buffer\n",
    "       self.buffer = BatchedReplayBuffer()\n",
    "\n",
    "\n",
    "       # TensorBoard Writer\n",
    "       logdir = f\"runs/hybrid_noisy_dqn_{datetime.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "       self.writer = tf.summary.create_file_writer(logdir)\n",
    "       print(f\"TensorBoard log directory: {logdir}\")\n",
    "\n",
    "\n",
    "       # Training step counter\n",
    "       self.train_step_count = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"train_steps\")\n",
    "\n",
    "\n",
    "   @tf.function\n",
    "   def train_step(self, S_grid, S_coord, A, R, S2_grid, S2_coord, D):\n",
    "       \"\"\"Performs a single training step using Double DQN update.\"\"\"\n",
    "\n",
    "\n",
    "       # --- Target Q-value Calculation (Double DQN) ---\n",
    "       # 1. Get next actions from the *online* model for S2\n",
    "       Q2_online = self.model([S2_grid, S2_coord], training=True) # Pass training=True for noisy layers\n",
    "       best_actions_next = tf.argmax(Q2_online, axis=1, output_type=tf.int32) # Shape [batch_size]\n",
    "\n",
    "\n",
    "       # 2. Get Q-values from the *target* model for S2\n",
    "       Q2_target = self.target([S2_grid, S2_coord], training=True) # Pass training=True for noisy layers\n",
    "\n",
    "\n",
    "       # 3. Select the Q-value from the target network corresponding to the best action selected by the online network\n",
    "       # Create indices: [[0, action0], [1, action1], ...]\n",
    "       action_indices = tf.stack([tf.range(tf.shape(best_actions_next)[0], dtype=tf.int32), best_actions_next], axis=1)\n",
    "       Q2_best_target = tf.gather_nd(Q2_target, action_indices) # Shape [batch_size]\n",
    "\n",
    "\n",
    "       # 4. Calculate the TD target: R + gamma * Q_target(S', argmax_a Q_online(S', a)) * (1 - D)\n",
    "       target_Q = R + self.gamma * Q2_best_target * (1.0 - tf.cast(D, tf.float32))\n",
    "       # --- End Target Q-value Calculation ---\n",
    "\n",
    "\n",
    "       # --- Loss Calculation and Gradient Update ---\n",
    "       with tf.GradientTape() as tape:\n",
    "           # Predict Q-values for the original states (S) using the online model\n",
    "           Q_online = self.model([S_grid, S_coord], training=True) # Pass training=True for noisy layers\n",
    "\n",
    "\n",
    "           # Select the Q-values corresponding to the actions actually taken (A)\n",
    "           # Create indices: [[0, A0], [1, A1], ...]\n",
    "           action_indices_taken = tf.stack([tf.range(tf.shape(A)[0], dtype=tf.int32), A], axis=1)\n",
    "           Q_online_taken = tf.gather_nd(Q_online, action_indices_taken) # Shape [batch_size]\n",
    "\n",
    "\n",
    "           # Calculate loss (e.g., Mean Squared Error) between target_Q and Q_online_taken\n",
    "           loss = tf.keras.losses.MeanSquaredError()(target_Q, Q_online_taken) # Use instantiated loss\n",
    "\n",
    "\n",
    "       # Compute and apply gradients\n",
    "       grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "       # Optional: Gradient clipping\n",
    "       # grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "       self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "       # --- End Loss Calculation and Gradient Update ---\n",
    "\n",
    "\n",
    "       # --- Soft Update Target Network ---\n",
    "       # Using tf.keras.layers.Layer.set_weights is generally safer and simpler than manual assignment loop\n",
    "       updated_target_weights = []\n",
    "       for w_online, w_target in zip(self.model.weights, self.target.weights):\n",
    "           updated_target_weights.append(self.tau * w_online + (1.0 - self.tau) * w_target)\n",
    "       self.target.set_weights(updated_target_weights)\n",
    "       # --- End Soft Update Target Network ---\n",
    "\n",
    "\n",
    "       return loss\n",
    "\n",
    "\n",
    "   @tf.function\n",
    "   def act_batch(self, S_tuple, deterministic=False):\n",
    "       \"\"\"Selects actions for a batch of states.\"\"\"\n",
    "       # Pass training=not deterministic to enable/disable noise in NoisyDense layers\n",
    "       # If deterministic (evaluation), training=False -> use mean weights, no noise\n",
    "       # If not deterministic (training), training=True -> use noisy weights\n",
    "       q_values = self.model(S_tuple, training=not deterministic)\n",
    "       actions = tf.argmax(q_values, axis=1, output_type=tf.int32)\n",
    "       return actions\n",
    "\n",
    "\n",
    "   def remember_batch(self, S_tuple, A, R, S2_tuple, D):\n",
    "       # Simply add the batch of transitions to the buffer\n",
    "       self.buffer.add_batch(S_tuple, A, R, S2_tuple, D)\n",
    "\n",
    "\n",
    "   def learn(self, batch_size=32):\n",
    "       \"\"\"Samples from buffer and performs a training step.\"\"\"\n",
    "       # Check buffer size before sampling\n",
    "       if len(self.buffer) < batch_size:\n",
    "           return None # Not enough samples to learn\n",
    "\n",
    "\n",
    "       # Sample data\n",
    "       sampled_data = self.buffer.sample(batch_size)\n",
    "       if sampled_data is None: # Should not happen if len check passes, but safe check\n",
    "           return None\n",
    "\n",
    "\n",
    "       # Unpack sampled data\n",
    "       S_grid_s, S_coord_s, A_s, R_s, S2_grid_s, S2_coord_s, D_s = sampled_data\n",
    "\n",
    "\n",
    "       # Perform the training step\n",
    "       loss = self.train_step(S_grid_s, S_coord_s, A_s, R_s, S2_grid_s, S2_coord_s, D_s)\n",
    "\n",
    "\n",
    "       # Log loss to TensorBoard\n",
    "       step = self.train_step_count.numpy() # Get current step value\n",
    "       with self.writer.as_default(step=step):\n",
    "           tf.summary.scalar(\"Train/Loss\", loss)\n",
    "\n",
    "\n",
    "       # Increment training step counter\n",
    "       self.train_step_count.assign_add(1)\n",
    "\n",
    "\n",
    "       return loss.numpy() # Return scalar loss value\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Evaluation Function (Moved Before Training Loop & Returns Stats)\n",
    "# -----------------------------------------------------------------------------\n",
    "def evaluate_agent_performance(agent, grid_size, max_steps, num_eval_episodes=10, render=True, render_env_index=0):\n",
    "   \"\"\"\n",
    "   Evaluates a trained agent deterministically, calculates performance statistics,\n",
    "   and optionally renders the final state of one specified environment.\n",
    "   Handles hybrid state (grid, coords). Returns key statistics.\n",
    "   \"\"\"\n",
    "   print(f\"\\n--- Running Evaluation ({num_eval_episodes} episodes) ---\")\n",
    "   eval_start_time = time.time()\n",
    "\n",
    "\n",
    "   # Create a separate environment instance for evaluation\n",
    "   eval_env = BatchedSculpt3DEnvTF(grid_size=grid_size, max_steps=max_steps, n_envs=num_eval_episodes)\n",
    "\n",
    "\n",
    "   # Calculate initial carvable count (do this once)\n",
    "   initial_shape_mask_flat_gpu = eval_env.shape_mask[0] # Use env 0 as reference\n",
    "   initial_shape_mask_flat_np = initial_shape_mask_flat_gpu.numpy()\n",
    "   initial_carvable_mask_flat = ~initial_shape_mask_flat_np # Material outside the shape\n",
    "   initial_carvable_count = np.sum(initial_carvable_mask_flat)\n",
    "   print(f\"  Initial number of carvable voxels: {initial_carvable_count}\")\n",
    "   if initial_carvable_count == 0: print(\"  Warning: No carvable material defined.\")\n",
    "\n",
    "\n",
    "   # Lists to store results across all evaluation episodes\n",
    "   all_ep_rewards = []\n",
    "   all_ep_lengths = []\n",
    "   all_ep_removed_counts = []\n",
    "   all_ep_incorrect_removed_counts = []\n",
    "\n",
    "\n",
    "   # Get the final stock state variable from the eval env *before* the loop\n",
    "   # This variable will be updated in-place by env.step\n",
    "   final_stock_variable_eval = eval_env.stock\n",
    "\n",
    "\n",
    "   # Run evaluation episodes\n",
    "   obs_tuple = eval_env.reset()\n",
    "   done = tf.zeros([num_eval_episodes], tf.bool)\n",
    "   ep_rewards = tf.zeros([num_eval_episodes], tf.float32)\n",
    "   ep_steps = tf.zeros([num_eval_episodes], tf.int32)\n",
    "\n",
    "\n",
    "   for _ in range(max_steps): # Limit loop by max_steps\n",
    "        if tf.reduce_all(done): break # Exit early if all envs are done\n",
    "\n",
    "        # Get actions deterministically (no noise)\n",
    "        A = agent.act_batch(obs_tuple, deterministic=True)\n",
    "\n",
    "        # Step the environment\n",
    "        S2_tuple, R, next_done = eval_env.step(A)\n",
    "\n",
    "        # Update rewards and steps only for environments not yet done\n",
    "        active_mask = ~done\n",
    "        ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "        ep_steps += tf.cast(active_mask, tf.int32)\n",
    "\n",
    "        # Update observations and done status\n",
    "        obs_tuple = S2_tuple\n",
    "        done = next_done # Use the done flags returned by step\n",
    "\n",
    "\n",
    "   # After the loop, collect final results\n",
    "   final_stock_batch_np = final_stock_variable_eval.numpy() # Get final stock state\n",
    "   batch_rewards = ep_rewards.numpy()\n",
    "   batch_lengths = ep_steps.numpy()\n",
    "\n",
    "\n",
    "   all_ep_rewards.extend(batch_rewards.tolist())\n",
    "   all_ep_lengths.extend(batch_lengths.tolist())\n",
    "\n",
    "\n",
    "   # Calculate removed/incorrect counts per episode\n",
    "   for i in range(num_eval_episodes):\n",
    "       final_stock_flat_np = final_stock_batch_np[i]\n",
    "\n",
    "\n",
    "       # Correctly removed: Initially carvable AND now gone\n",
    "       removed_mask = initial_carvable_mask_flat & (~final_stock_flat_np)\n",
    "       removed_count = np.sum(removed_mask)\n",
    "       all_ep_removed_counts.append(removed_count)\n",
    "\n",
    "\n",
    "       # Incorrectly removed: Initially part of shape AND now gone\n",
    "       incorrectly_removed_mask = initial_shape_mask_flat_np & (~final_stock_flat_np)\n",
    "       incorrectly_removed_count = np.sum(incorrectly_removed_mask)\n",
    "       all_ep_incorrect_removed_counts.append(incorrectly_removed_count)\n",
    "\n",
    "\n",
    "   # Aggregate and Print Statistics\n",
    "   avg_reward = np.mean(all_ep_rewards); std_reward = np.std(all_ep_rewards)\n",
    "   avg_length = np.mean(all_ep_lengths)\n",
    "   avg_removed_count = np.mean(all_ep_removed_counts)\n",
    "   avg_incorrect_removed = np.mean(all_ep_incorrect_removed_counts)\n",
    "\n",
    "\n",
    "   # Calculate removal percentage safely\n",
    "   if initial_carvable_count > 0:\n",
    "       removal_percentages = [(c / initial_carvable_count) * 100.0 for c in all_ep_removed_counts]\n",
    "       avg_removal_percentage = np.mean(removal_percentages)\n",
    "       std_removal_percentage = np.std(removal_percentages)\n",
    "   else:\n",
    "       avg_removal_percentage = 0.0\n",
    "       std_removal_percentage = 0.0\n",
    "\n",
    "\n",
    "   print(f\"\\n--- Evaluation Results ---\")\n",
    "   print(f\"  Avg Reward : {avg_reward:.2f} (+/- {std_reward:.2f})\")\n",
    "   print(f\"  Avg Length : {avg_length:.1f}\")\n",
    "   print(f\"  Avg Removed: {avg_removed_count:.1f} / {initial_carvable_count} ({avg_removal_percentage:.2f}% +/- {std_removal_percentage:.2f}%)\")\n",
    "   print(f\"  Avg Incorrect: {avg_incorrect_removed:.1f}\")\n",
    "   eval_duration = time.time() - eval_start_time\n",
    "   print(f\"  Evaluation Duration: {eval_duration:.2f}s\")\n",
    "\n",
    "\n",
    "   # Render Final State\n",
    "   if render:\n",
    "       print(f\"\\n--- Rendering final state for Eval Env Index: {render_env_index} ---\")\n",
    "       if render_env_index < 0 or render_env_index >= num_eval_episodes:\n",
    "            print(f\"Error: render_env_index ({render_env_index}) out of bounds for {num_eval_episodes} eval envs.\")\n",
    "       elif num_eval_episodes > 0:\n",
    "           try:\n",
    "               # Get final stock and shape mask for the specific environment\n",
    "               final_stock_flat_np = final_stock_batch_np[render_env_index]\n",
    "               shape_mask_flat_np = initial_shape_mask_flat_np # Same for all\n",
    "\n",
    "\n",
    "               G = grid_size\n",
    "               final_stock_3d = final_stock_flat_np.reshape((G, G, G))\n",
    "               shape_mask_3d = shape_mask_flat_np.reshape((G, G, G))\n",
    "\n",
    "\n",
    "               # Masks for plotting\n",
    "               shape_to_plot = shape_mask_3d # Target shape\n",
    "               initial_carvable_mask_render = ~shape_mask_3d # Initially carvable\n",
    "               removed_mask_render = initial_carvable_mask_render & (~final_stock_3d) # Correctly removed\n",
    "               incorrectly_removed_mask_render = shape_mask_3d & (~final_stock_3d) # Incorrectly removed\n",
    "\n",
    "\n",
    "               # Plotting\n",
    "               fig = plt.figure(figsize=(9, 7)); ax = fig.add_subplot(111, projection='3d')\n",
    "               ax.set_facecolor('whitesmoke')\n",
    "               # Coordinates for voxels (need +1 for boundaries)\n",
    "               x_vox, y_vox, z_vox = np.indices(np.array(shape_to_plot.shape) + 1)\n",
    "\n",
    "\n",
    "               # Plot target shape (transparent blue)\n",
    "               ax.voxels(x_vox, y_vox, z_vox, shape_to_plot, facecolors='blue', alpha=0.1, edgecolor=None)\n",
    "\n",
    "\n",
    "               # Plot correctly removed material (solid red)\n",
    "               ax.voxels(x_vox, y_vox, z_vox, removed_mask_render, facecolors='red', alpha=0.6, edgecolor=None)\n",
    "\n",
    "\n",
    "               # Plot incorrectly removed material (yellow, if any)\n",
    "               if np.sum(incorrectly_removed_mask_render) > 0:\n",
    "                    ax.voxels(x_vox, y_vox, z_vox, incorrectly_removed_mask_render, facecolors='yellow', alpha=0.7, edgecolor='orange', label='Incorrect Removal')\n",
    "                    ax.legend()\n",
    "\n",
    "\n",
    "               # Set title and labels\n",
    "               rendered_env_reward = all_ep_rewards[render_env_index]\n",
    "               rendered_env_removed = all_ep_removed_counts[render_env_index]\n",
    "               rendered_env_perc = (rendered_env_removed / initial_carvable_count) * 100.0 if initial_carvable_count > 0 else 0.0\n",
    "               ax.set_title(f\"Eval Render Env #{render_env_index} (R={rendered_env_reward:.1f}, Removed={rendered_env_perc:.1f}%)\")\n",
    "               ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "               ax.set_xlim(0, G); ax.set_ylim(0, G); ax.set_zlim(0, G); ax.set_aspect('auto') # Use 'auto' or 'equal'\n",
    "               plt.tight_layout()\n",
    "\n",
    "\n",
    "               # Save the plot\n",
    "               render_dir = \"renders_eval\"\n",
    "               os.makedirs(render_dir, exist_ok=True) # Ensure directory exists\n",
    "               save_path = os.path.join(render_dir, f\"eval_render_env_{render_env_index}_final.png\")\n",
    "               plt.savefig(save_path); print(f\"Saved evaluation render to {save_path}\"); plt.close(fig)\n",
    "           except Exception as e:\n",
    "               print(f\"Error during rendering: {e}\")\n",
    "               import traceback; traceback.print_exc()\n",
    "\n",
    "\n",
    "   # Return aggregated statistics\n",
    "   return {\n",
    "       \"avg_reward\": avg_reward,\n",
    "       \"std_reward\": std_reward,\n",
    "       \"avg_length\": avg_length,\n",
    "       \"avg_removed_count\": avg_removed_count,\n",
    "       \"avg_incorrect_removed\": avg_incorrect_removed,\n",
    "       \"avg_removal_percentage\": avg_removal_percentage,\n",
    "       \"std_removal_percentage\": std_removal_percentage,\n",
    "       \"initial_carvable_count\": initial_carvable_count\n",
    "   }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Training Loop (Using Hybrid State & Noisy Agent) with Periodic Saving\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_gpu_batched(\n",
    "       grid_size=16, max_steps=300, n_envs=32, episodes=10000,\n",
    "       buffer_capacity=100000, learn_batch_size=32, learn_freq=4,\n",
    "       gamma=0.99, lr=1e-4, tau=0.005, log_every=50,\n",
    "       evaluate_every=100,\n",
    "       num_eval_episodes_periodic=10,\n",
    "       render_intermediate_eval=False,\n",
    "       # <<< NEW Parameters for Saving >>>\n",
    "       save_every_episodes=5,\n",
    "       checkpoint_dir=\"checkpoints\"\n",
    "       ):\n",
    "\n",
    "\n",
    "   print(f\"--- Training Hybrid Noisy DQN Agent ---\")\n",
    "   print(f\"Params: Grid={grid_size}, N_Envs={n_envs}, MaxSteps={max_steps}, Episodes={episodes}\")\n",
    "   print(f\"Learn Batch Size (B): {learn_batch_size}, Total samples/learn step (B*N): {learn_batch_size * n_envs}\")\n",
    "   print(f\"Periodic Evaluation every {evaluate_every} episodes ({num_eval_episodes_periodic} eps each).\")\n",
    "   print(f\"Periodic Weight Saving every {save_every_episodes} episodes to '{checkpoint_dir}'.\") # <<< Log saving info\n",
    "   print(f\"Memory Warning: Ensure sufficient CPU RAM and GPU VRAM.\")\n",
    "\n",
    "\n",
    "   env = BatchedSculpt3DEnvTF(grid_size, max_steps, n_envs)\n",
    "   agent = BatchedDQNAgentTF(grid_shape=env.grid_obs_shape, coord_shape=env.coord_obs_shape,\n",
    "                             action_dim=6, lr=lr, gamma=gamma, tau=tau)\n",
    "   agent.buffer.cap = buffer_capacity # Set buffer capacity\n",
    "\n",
    "\n",
    "   total_steps_taken = 0\n",
    "   episode_rewards_history = []\n",
    "   episode_lengths_history = []\n",
    "   start_time = time.time()\n",
    "\n",
    "\n",
    "   # Lists to store evaluation results for plotting trend\n",
    "   eval_episodes_list = []\n",
    "   eval_avg_rewards_list = []\n",
    "   eval_avg_removal_perc_list = []\n",
    "\n",
    "\n",
    "   # --- Main Training Loop ---\n",
    "   for ep in range(1, episodes + 1):\n",
    "       obs_tuple = env.reset()\n",
    "       done = tf.zeros([n_envs], tf.bool)\n",
    "       ep_rewards = tf.zeros([n_envs], tf.float32)\n",
    "       ep_steps = tf.zeros([n_envs], tf.int32)\n",
    "\n",
    "\n",
    "       # --- Episode Step Loop ---\n",
    "       # Limit by max_steps to prevent infinite loops if agent gets stuck\n",
    "       for current_ep_step in range(max_steps):\n",
    "           if tf.reduce_all(done): break # Exit if all envs finished\n",
    "\n",
    "\n",
    "           # Act (use noise during training)\n",
    "           A = agent.act_batch(obs_tuple, deterministic=False)\n",
    "\n",
    "\n",
    "           # Step environment\n",
    "           S2_tuple, R, next_done = env.step(A)\n",
    "\n",
    "\n",
    "           # Remember transition\n",
    "           agent.remember_batch(obs_tuple, A, R, S2_tuple, next_done)\n",
    "\n",
    "\n",
    "           # Update episode stats for active environments\n",
    "           active_mask = ~done\n",
    "           ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "           ep_steps += tf.cast(active_mask, tf.int32)\n",
    "\n",
    "\n",
    "           # Update state and done flags\n",
    "           obs_tuple = S2_tuple\n",
    "           done = next_done # Use done flags from the step\n",
    "\n",
    "\n",
    "           # Update total steps taken\n",
    "           total_steps_taken += n_envs\n",
    "\n",
    "\n",
    "           # Learn periodically based on steps taken within the episode (or total steps)\n",
    "           # Using total_steps_taken might be more common\n",
    "           if total_steps_taken > 0 and total_steps_taken % (learn_freq * n_envs) == 0: # Learn every 'learn_freq' agent steps approx\n",
    "               loss_val = agent.learn(learn_batch_size)\n",
    "       # --- End Episode Step Loop ---\n",
    "\n",
    "\n",
    "       # Calculate and store average batch stats\n",
    "       avg_reward_batch = tf.reduce_mean(ep_rewards).numpy()\n",
    "       avg_steps_batch = tf.reduce_mean(tf.cast(ep_steps, tf.float32)).numpy()\n",
    "       episode_rewards_history.append(avg_reward_batch)\n",
    "       episode_lengths_history.append(avg_steps_batch)\n",
    "\n",
    "\n",
    "       # --- Logging ---\n",
    "       if ep % log_every == 0 or ep == 1:\n",
    "           elapsed_time = time.time() - start_time\n",
    "           # Calculate rolling averages for smoother logging\n",
    "           avg_r = np.mean(episode_rewards_history[-log_every:]) if len(episode_rewards_history) >= log_every else np.mean(episode_rewards_history)\n",
    "           avg_l = np.mean(episode_lengths_history[-log_every:]) if len(episode_lengths_history) >= log_every else np.mean(episode_lengths_history)\n",
    "           print(f\"Ep {ep}/{episodes} | Avg R (last {log_every}): {avg_r:.2f} | Avg Len: {avg_l:.1f} | Steps: {total_steps_taken} | TrainSteps: {agent.train_step_count.numpy()} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "\n",
    "           # Log scalars to TensorBoard\n",
    "           agent_step = agent.train_step_count.numpy()\n",
    "           with agent.writer.as_default(step=agent_step):\n",
    "                tf.summary.scalar(\"Episode/AvgReward_Roll\", avg_r)\n",
    "                tf.summary.scalar(\"Episode/AvgLength_Roll\", avg_l)\n",
    "                tf.summary.scalar(\"System/TotalEnvSteps\", total_steps_taken)\n",
    "\n",
    "\n",
    "           # --- TensorBoard Image Logging (Render one environment) ---\n",
    "           try:\n",
    "                render_env_index = 0\n",
    "                # Get current stock and shape for the specific env as numpy arrays\n",
    "                stock_np = env.stock.numpy()[render_env_index].reshape((grid_size, grid_size, grid_size))\n",
    "                shape_np = env.shape_mask.numpy()[render_env_index].reshape((grid_size, grid_size, grid_size))\n",
    "\n",
    "\n",
    "                # Calculate masks for rendering\n",
    "                removed_mask = (~shape_np) & (~stock_np) # Initially carvable and now gone\n",
    "                incorrect_mask = shape_np & (~stock_np)  # Initially shape and now gone\n",
    "\n",
    "\n",
    "                # Create plot\n",
    "                fig = plt.figure(figsize=(6, 5))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                x_vox, y_vox, z_vox = np.indices(np.array(stock_np.shape) + 1)\n",
    "\n",
    "\n",
    "                # Plot volumes\n",
    "                ax.voxels(x_vox, y_vox, z_vox, shape_np, facecolors='blue', alpha=0.1) # Target shape\n",
    "                ax.voxels(x_vox, y_vox, z_vox, removed_mask, facecolors='red', alpha=0.6) # Correctly removed\n",
    "                if np.sum(incorrect_mask) > 0:\n",
    "                    ax.voxels(x_vox, y_vox, z_vox, incorrect_mask, facecolors='yellow', alpha=0.7) # Incorrectly removed\n",
    "\n",
    "\n",
    "                ax.set_title(f\"Ep {ep} - Render Env #{render_env_index}\")\n",
    "                ax.set_axis_off() # Clean look for TensorBoard\n",
    "                fig.tight_layout()\n",
    "\n",
    "\n",
    "                # Convert plot to PNG image bytes\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                buf.seek(0)\n",
    "                # Decode PNG and add batch dimension for TensorBoard\n",
    "                image_tensor = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "                image_tensor = tf.expand_dims(image_tensor, 0)\n",
    "                buf.close()\n",
    "                plt.close(fig) # Close plot to free memory\n",
    "\n",
    "\n",
    "                # Write image to TensorBoard\n",
    "                with agent.writer.as_default(step=agent.train_step_count.numpy()):\n",
    "                    tf.summary.image(\"EnvRender/Env0_Train\", image_tensor)\n",
    "           except Exception as e:\n",
    "                print(f\"Render log error at ep {ep}: {e}\")\n",
    "       # --- End Logging ---\n",
    "\n",
    "\n",
    "       # <<< --- Periodic Model Saving --- >>>\n",
    "       if save_every_episodes > 0 and ep % save_every_episodes == 0 and ep > 0:\n",
    "           try:\n",
    "               # Ensure the checkpoint directory exists\n",
    "               os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "               # Construct save path\n",
    "               save_path = os.path.join(checkpoint_dir, f\"model_ep{ep}_g{grid_size}.weights.h5\")\n",
    "               # Save the weights of the online model\n",
    "               agent.model.save_weights(save_path)\n",
    "               print(f\"\\n--- Saved model weights at episode {ep} to {save_path} ---\")\n",
    "           except Exception as e:\n",
    "               print(f\"\\n--- Error saving weights at episode {ep}: {e} ---\")\n",
    "       # <<< --- End Periodic Model Saving --- >>>\n",
    "\n",
    "\n",
    "       # --- Periodic Evaluation ---\n",
    "       if evaluate_every > 0 and ep % evaluate_every == 0:\n",
    "           eval_stats = evaluate_agent_performance(\n",
    "               agent=agent,\n",
    "               grid_size=grid_size,\n",
    "               max_steps=max_steps,\n",
    "               num_eval_episodes=num_eval_episodes_periodic,\n",
    "               render=render_intermediate_eval,\n",
    "               render_env_index=0 # Render env 0 if rendering is enabled\n",
    "           )\n",
    "           # Store results for final trend plot\n",
    "           if eval_stats:\n",
    "               eval_episodes_list.append(ep)\n",
    "               eval_avg_rewards_list.append(eval_stats[\"avg_reward\"])\n",
    "               eval_avg_removal_perc_list.append(eval_stats[\"avg_removal_percentage\"])\n",
    "\n",
    "\n",
    "               # Log evaluation stats to TensorBoard\n",
    "               agent_step = agent.train_step_count.numpy()\n",
    "               with agent.writer.as_default(step=agent_step):\n",
    "                   tf.summary.scalar(\"Evaluate/AvgReward\", eval_stats[\"avg_reward\"])\n",
    "                   tf.summary.scalar(\"Evaluate/AvgRemovalPercentage\", eval_stats[\"avg_removal_percentage\"])\n",
    "                   tf.summary.scalar(\"Evaluate/AvgIncorrectRemoved\", eval_stats[\"avg_incorrect_removed\"])\n",
    "\n",
    "\n",
    "           print(\"-\" * 60) # Separator after evaluation output\n",
    "       # --- End Periodic Evaluation ---\n",
    "\n",
    "\n",
    "   # --- End of Training Loop ---\n",
    "   agent.writer.close() # Close the TensorBoard writer\n",
    "   total_training_time = time.time() - start_time\n",
    "   print(f\"\\nTraining finished. Total steps: {total_steps_taken}, Total Time: {total_training_time:.2f}s\")\n",
    "\n",
    "\n",
    "   # --- Plot Evaluation Trend ---\n",
    "   if eval_episodes_list:\n",
    "       print(\"\\n--- Plotting Evaluation Trend ---\")\n",
    "       try:\n",
    "           fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "\n",
    "           color = 'tab:red'\n",
    "           ax1.set_xlabel('Training Episode')\n",
    "           ax1.set_ylabel('Avg Carvable Material Removed (%)', color=color)\n",
    "           ax1.plot(eval_episodes_list, eval_avg_removal_perc_list, color=color, marker='o', linestyle='-', label='Removal %')\n",
    "           ax1.tick_params(axis='y', labelcolor=color)\n",
    "           ax1.grid(True, axis='y', linestyle=':')\n",
    "\n",
    "\n",
    "           ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\n",
    "           color = 'tab:blue'\n",
    "           ax2.set_ylabel('Avg Evaluation Reward', color=color)\n",
    "           ax2.plot(eval_episodes_list, eval_avg_rewards_list, color=color, marker='x', linestyle='--', label='Avg Reward')\n",
    "           ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "           fig.suptitle('Agent Evaluation Performance During Training')\n",
    "           fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
    "\n",
    "\n",
    "           # Create plot directory if it doesn't exist\n",
    "           plot_dir = \"plots\"\n",
    "           os.makedirs(plot_dir, exist_ok=True)\n",
    "           plot_path = os.path.join(plot_dir, f\"evaluation_trend_g{grid_size}_n{n_envs}.png\")\n",
    "           plt.savefig(plot_path)\n",
    "           print(f\"Saved evaluation trend plot to {plot_path}\")\n",
    "           plt.close(fig) # Close plot\n",
    "\n",
    "\n",
    "       except Exception as e:\n",
    "           print(f\"Error plotting evaluation trend: {e}\")\n",
    "\n",
    "\n",
    "   return agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Main Execution Block (Adjusted Parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "   # --- Parameters (Adjust based on your hardware!) ---\n",
    "   GRID_SIZE_RUN = 8         # Smaller grid for potentially faster runs\n",
    "   N_ENVS_RUN = 16           # Number of parallel environments\n",
    "   MAX_STEPS_RUN = 200       # Max steps per episode\n",
    "   EPISODES_RUN = 5000       # Total training episodes\n",
    "   BUFFER_CAP_RUN = 50000    # Replay buffer capacity (in batches)\n",
    "   LEARN_BATCH_RUN = 32      # Number of *batches* to sample for learning\n",
    "   LEARNING_RATE = 1e-4      # Learning rate for Adam\n",
    "   EVAL_FREQ_RUN = 50       # Evaluate every N training episodes\n",
    "   NUM_EVAL_EPISODES_RUN = 10 # Number of episodes per evaluation run\n",
    "   SAVE_FREQ_RUN = 1000       # <<< Save weights every N episodes >>>\n",
    "   CHECKPOINT_DIR_RUN = \"checkpoints_hybrid_noisy\" # <<< Directory for saved weights >>>\n",
    "\n",
    "\n",
    "   print(f\"Starting run with Hybrid State + Noisy Nets\")\n",
    "   print(f\"Params: Grid={GRID_SIZE_RUN}, N_Envs={N_ENVS_RUN}, LearnBatch(B)={LEARN_BATCH_RUN}\")\n",
    "   print(f\"Total samples per learn step (B*N): {LEARN_BATCH_RUN * N_ENVS_RUN}\")\n",
    "\n",
    "\n",
    "   # Train the agent, with periodic evaluation and saving\n",
    "   trained_agent = train_gpu_batched(\n",
    "       grid_size=GRID_SIZE_RUN,\n",
    "       max_steps=MAX_STEPS_RUN,\n",
    "       n_envs=N_ENVS_RUN,\n",
    "       episodes=EPISODES_RUN,\n",
    "       buffer_capacity=BUFFER_CAP_RUN,\n",
    "       learn_batch_size=LEARN_BATCH_RUN,\n",
    "       learn_freq=4, # Learn approx every 4 agent steps\n",
    "       gamma=0.99,\n",
    "       lr=LEARNING_RATE,\n",
    "       tau=0.005, # Target network update rate\n",
    "       log_every=5,\n",
    "       evaluate_every=EVAL_FREQ_RUN,\n",
    "       num_eval_episodes_periodic=NUM_EVAL_EPISODES_RUN,\n",
    "       render_intermediate_eval=False, # Keep intermediate rendering off by default\n",
    "       # <<< Pass saving parameters >>>\n",
    "       save_every_episodes=SAVE_FREQ_RUN,\n",
    "       checkpoint_dir=CHECKPOINT_DIR_RUN\n",
    "   )\n",
    "\n",
    "\n",
    "   # --- Run Final Evaluation After Training ---\n",
    "   print(\"\\n\" + \"=\"*70)\n",
    "   print(\"      RUNNING FINAL EVALUATION ON TRAINED AGENT\")\n",
    "   print(\"=\"*70)\n",
    "   if trained_agent:\n",
    "        evaluate_agent_performance(\n",
    "            agent=trained_agent,\n",
    "            grid_size=GRID_SIZE_RUN,\n",
    "            max_steps=MAX_STEPS_RUN,\n",
    "            num_eval_episodes=50, # Evaluate over more episodes for final assessment\n",
    "            render=True,          # Render the final plot for one episode\n",
    "            render_env_index=0\n",
    "        )\n",
    "\n",
    "\n",
    "   # ---  Save Final Model Weights ---\n",
    "   if trained_agent:\n",
    "       # Use the same checkpoint directory for the final save\n",
    "       final_save_path = os.path.join(CHECKPOINT_DIR_RUN, f\"FINAL_model_ep{EPISODES_RUN}_g{GRID_SIZE_RUN}.weights.h5\")\n",
    "       try:\n",
    "           # Ensure directory exists for final save too\n",
    "           os.makedirs(CHECKPOINT_DIR_RUN, exist_ok=True)\n",
    "           trained_agent.model.save_weights(final_save_path)\n",
    "           print(f\"\\nFinal model weights saved to {final_save_path}\")\n",
    "       except Exception as e:\n",
    "           print(f\"\\nError saving final model weights: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
