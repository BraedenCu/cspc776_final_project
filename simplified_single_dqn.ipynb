{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fully TensorFlow‑Accelerated Sculpt3DEnv + DQN Agent\n",
    "• Environment ops in @tf.function\n",
    "• All tensors stay in TF until logging, where we convert to Python floats/arrays\n",
    "• Extensive TensorBoard logging\n",
    "• Periodic 3D rendering of path + object\n",
    "\n",
    "Usage:\n",
    "  python sculpt_tf_dqn.py\n",
    "  tensorboard --logdir=runs\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "\n",
    "# --- 1) Sculpt3DEnvTF ---\n",
    "class Sculpt3DEnvTF:\n",
    "    def __init__(self, grid_size=20, max_steps=200):\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # stock & shape on CPU to leverage gather_nd there\n",
    "        with tf.device('/CPU:0'):\n",
    "            self.stock = tf.Variable(tf.ones([grid_size,grid_size,grid_size], dtype=tf.bool), trainable=False)\n",
    "            coords = tf.stack(tf.meshgrid(\n",
    "                tf.range(grid_size), tf.range(grid_size), tf.range(grid_size),\n",
    "                indexing='ij'), axis=-1)\n",
    "            center = tf.constant([grid_size//2]*3, dtype=tf.int32)\n",
    "            dist2 = tf.reduce_sum(tf.square(tf.cast(coords - center, tf.int32)), axis=-1)\n",
    "            r = tf.cast(grid_size//2 - 1, tf.int32)\n",
    "            self.shape = tf.Variable(dist2 <= r*r, trainable=False)\n",
    "\n",
    "        # router & control on default device (GPU)\n",
    "        self.router_pos = tf.Variable([0,0,0], dtype=tf.int32)\n",
    "        self.steps      = tf.Variable(0,          dtype=tf.int32)\n",
    "        self.done       = tf.Variable(False,      dtype=tf.bool)\n",
    "\n",
    "        # Pre‑defined moves\n",
    "        self.moves = tf.constant(\n",
    "            [[1,0,0],[-1,0,0],[0,1,0],[0,-1,0],[0,0,1],[0,0,-1]],\n",
    "            dtype=tf.int32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        # reset stock on CPU\n",
    "        with tf.device('/CPU:0'):\n",
    "            self.stock.assign(tf.ones_like(self.stock))\n",
    "\n",
    "        self.steps.assign(0)\n",
    "        self.done.assign(False)\n",
    "\n",
    "        # pick random start outside protected shape\n",
    "        flat_mask = (~self.shape).numpy().reshape(-1)\n",
    "        choices = np.nonzero(flat_mask)[0]\n",
    "        idx = random.choice(choices)\n",
    "        z = idx % self.grid_size\n",
    "        y = (idx // self.grid_size) % self.grid_size\n",
    "        x = idx // (self.grid_size * self.grid_size)\n",
    "        self.router_pos.assign([x,y,z])\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, action):\n",
    "        move   = tf.gather(self.moves, action)\n",
    "        newpos = self.router_pos + move\n",
    "        inb    = tf.reduce_all((newpos>=0)&(newpos<self.grid_size))\n",
    "\n",
    "        def _oob():\n",
    "            self.done.assign(True)\n",
    "            return tf.constant(-5.0)\n",
    "        def _carve():\n",
    "            idxs = tf.stack([self.router_pos, newpos], axis=0)\n",
    "            with tf.device('/CPU:0'):\n",
    "                shape_vals = tf.gather_nd(self.shape, idxs)\n",
    "            def _hit():\n",
    "                self.done.assign(True)\n",
    "                return tf.constant(-5.0)\n",
    "            def _remove():\n",
    "                with tf.device('/CPU:0'):\n",
    "                    stock_vals = tf.gather_nd(self.stock, idxs)\n",
    "                    updated    = tf.tensor_scatter_nd_update(self.stock, idxs, tf.zeros([2],dtype=tf.bool))\n",
    "                    self.stock.assign(updated)\n",
    "                self.router_pos.assign(newpos)\n",
    "                return tf.reduce_sum(tf.cast(stock_vals, tf.float32))\n",
    "            return tf.cond(tf.reduce_any(shape_vals), _hit, _remove)\n",
    "\n",
    "        reward = tf.cond(inb, _carve, _oob)\n",
    "        self.steps.assign_add(1)\n",
    "        reward = reward - 0.1\n",
    "        self.done.assign(tf.logical_or(self.done, self.steps>=self.max_steps))\n",
    "\n",
    "        return self._get_obs(), reward, self.done\n",
    "\n",
    "    @tf.function\n",
    "    def _get_obs(self):\n",
    "        rx,ry,rz = tf.unstack(self.router_pos)\n",
    "        c = self.grid_size // 2\n",
    "        return tf.cast(tf.stack([rx,ry,rz, c,c,c]), tf.float32)\n",
    "\n",
    "\n",
    "# --- 2) TF Replay Buffer ---\n",
    "class ReplayBufferTF:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buf = []\n",
    "        self.cap = capacity\n",
    "\n",
    "    def add(self, s,a,r,ns,d):\n",
    "        if len(self.buf) >= self.cap:\n",
    "            self.buf.pop(0)\n",
    "        self.buf.append((s,a,r,ns,d))\n",
    "\n",
    "    def sample(self, bs):\n",
    "        batch = random.sample(self.buf, bs)\n",
    "        s,a,r,ns,d = zip(*batch)\n",
    "        return (\n",
    "            tf.stack(s),\n",
    "            tf.convert_to_tensor(a,  tf.int32),\n",
    "            tf.convert_to_tensor(r,  tf.float32),\n",
    "            tf.stack(ns),\n",
    "            tf.convert_to_tensor(d,  tf.bool)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "\n",
    "# --- 3) TF DQN Agent with Extensive Logging ---\n",
    "class DQNAgentTF:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, tau=0.01):\n",
    "        self.gamma, self.tau = gamma, tau\n",
    "\n",
    "        # build model & target\n",
    "        inputs = tf.keras.Input(shape=(state_dim,))\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        outputs = tf.keras.layers.Dense(action_dim)(x)\n",
    "        self.model  = tf.keras.Model(inputs, outputs)\n",
    "        self.target = tf.keras.models.clone_model(self.model)\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        self.buffer    = ReplayBufferTF()\n",
    "\n",
    "        # TensorBoard\n",
    "        logdir = f\"runs/tf_sculpt_full_{datetime.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "        self.writer = SummaryWriter(logdir)\n",
    "        self.step   = 0\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q     = self.model(states, training=True)\n",
    "            q_sa  = tf.reduce_sum(q * tf.one_hot(actions, tf.shape(q)[1]), axis=1)\n",
    "            qn    = self.target(next_states, training=False)\n",
    "            max_n = tf.reduce_max(qn, axis=1)\n",
    "            target= rewards + self.gamma * max_n * (1 - tf.cast(dones,tf.float32))\n",
    "            loss  = tf.reduce_mean(tf.square(q_sa - target))\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        grad_norm = tf.linalg.global_norm(grads)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # soft update\n",
    "        for w, tw in zip(self.model.weights, self.target.weights):\n",
    "            tw.assign(self.tau*w + (1-self.tau)*tw)\n",
    "\n",
    "        return loss, grad_norm\n",
    "\n",
    "    def remember(self, *args):\n",
    "        self.buffer.add(*args)\n",
    "\n",
    "    def act(self, state, eps=0.1):\n",
    "        if random.random() < eps:\n",
    "            return random.randrange(self.model.output_shape[-1])\n",
    "        qv = self.model(tf.expand_dims(state,0), training=False)[0]\n",
    "        noise = tf.random.normal(tf.shape(qv), stddev=eps)\n",
    "        return int(tf.argmax(qv + noise).numpy())\n",
    "\n",
    "    def learn(self, batch_size=64):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        s,a,r,ns,d = self.buffer.sample(batch_size)\n",
    "        loss, grad_norm = self.train_step(s,a,r,ns,d)\n",
    "\n",
    "        # log scalars\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar(\"Train/Loss\",        loss.numpy(),      self.step)\n",
    "        self.writer.add_scalar(\"Train/GradNorm\",    grad_norm.numpy(), self.step)\n",
    "        #self.writer.add_scalar(\"Episode/LearningRate\",\n",
    "        #               float(agent.optimizer.learning_rate.numpy()), ep)\n",
    "\n",
    "        # log histograms\n",
    "        for var in self.model.trainable_variables:\n",
    "            self.writer.add_histogram(var.name.replace(':','_'),\n",
    "                                      var.numpy(), self.step)\n",
    "\n",
    "\n",
    "# --- 4) Training Loop with Episode‐level Logging & Rendering ---\n",
    "def train_tf(env, episodes=500, eps_start=1.0, eps_end=0.05, eps_decay=0.995,\n",
    "             render_every=100):\n",
    "    state_dim, action_dim = 6, 6\n",
    "    agent = DQNAgentTF(state_dim, action_dim)\n",
    "    eps   = eps_start\n",
    "    recent_rewards = deque(maxlen=20)\n",
    "\n",
    "    for ep in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done  = False\n",
    "        total = 0.0\n",
    "        path  = []\n",
    "\n",
    "        while not done:\n",
    "            path.append(state[:3].numpy().tolist())\n",
    "            a = agent.act(state, eps)\n",
    "            ns, r, done = env.step(a)\n",
    "            agent.remember(state, a, r, ns, done)\n",
    "            agent.learn()\n",
    "            state, total = ns, total + r\n",
    "\n",
    "        recent_rewards.append(total)\n",
    "        avg20 = float(np.mean(recent_rewards))\n",
    "\n",
    "        # episode scalars\n",
    "        agent.writer.add_scalar(\"Episode/Reward\",     float(total), ep)\n",
    "        agent.writer.add_scalar(\"Episode/Avg20Reward\", avg20,        ep)\n",
    "        agent.writer.add_scalar(\"Episode/Epsilon\",     eps,          ep)\n",
    "        agent.writer.add_scalar(\"Episode/LearningRate\",\n",
    "                       float(agent.optimizer.learning_rate.numpy()), ep)\n",
    "\n",
    "        # periodic 3D render\n",
    "        if ep % render_every == 0:\n",
    "            fig = plt.figure()\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            xs, ys, zs = zip(*path)\n",
    "            ax.plot(xs, ys, zs, '-o', label=f\"Ep{ep} R={total:.2f}\")\n",
    "\n",
    "            # plot protected shape\n",
    "            mask = env.shape.numpy()\n",
    "            pts = np.argwhere(mask)\n",
    "            ax.scatter(pts[:,0], pts[:,1], pts[:,2], s=1, alpha=0.1, color='red')\n",
    "\n",
    "            ax.set_title(f\"Episode {ep}\")\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        print(f\"Ep {ep}/{episodes}  R={total:.2f}  Avg20={avg20:.2f}  Eps={eps:.3f}\")\n",
    "\n",
    "    agent.writer.close()\n",
    "    return agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Sculpt3DEnvTF(grid_size=20, max_steps=200)\n",
    "    train_tf(env, episodes=1000, render_every=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
