{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fully‑GPU Batched Sculpt3DEnv + DQN Agent\n",
    "• N envs stepped in parallel (no Python loops per step)\n",
    "• Paths pre‑seeded with starting pos to avoid empty unpack errors  <-- NOTE: Path tracking modified\n",
    "• Batched replay buffer insertion\n",
    "• Periodic 3D rendering, guarded against empty paths <-- NOTE: Rendering needs adaptation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# Suppress TensorFlow INFO/WARNING messages\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Temporarily comment out for more verbose TF logs if needed\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "# from torch.utils.tensorboard import SummaryWriter # Replaced with tf.summary\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import time # For basic timing\n",
    "\n",
    "# Optional: Explicitly check for GPU and log device placement\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True) # Uncomment for verbose device placement logs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Batched Sculpt3DEnvTF\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedSculpt3DEnvTF:\n",
    "    # Increased default grid size slightly for potentially more interesting tasks\n",
    "    def __init__(self, grid_size=32, max_steps=200, n_envs=16):\n",
    "        G, N = grid_size, n_envs # G is a Python int here\n",
    "        self.G, self.N, self.max_steps = G, N, max_steps\n",
    "        self.flat_dim = G*G*G\n",
    "\n",
    "        # Use float32 for coords calculation to avoid potential type issues downstream\n",
    "        coords_range = tf.range(G, dtype=tf.float32)\n",
    "        coords = tf.stack(tf.meshgrid(\n",
    "            coords_range, coords_range, coords_range,\n",
    "            indexing='ij'\n",
    "        ), axis=-1)  # [G,G,G,3]\n",
    "\n",
    "        # Ensure center calculation handles odd/even G correctly\n",
    "        center = tf.constant([G/2 - 0.5, G/2 - 0.5, G/2 - 0.5], tf.float32)\n",
    "        # Calculate squared distance\n",
    "        dist2 = tf.reduce_sum(tf.square(coords - center), axis=-1)\n",
    "        # Use float comparison for radius\n",
    "        radius_sq = tf.square(tf.cast(G // 2 - 1, tf.float32))\n",
    "        mask3d = dist2 <= radius_sq\n",
    "        mask_flat = tf.reshape(mask3d, [-1])  # [G^3] boolean\n",
    "\n",
    "        # Place variables on GPU if available, TF handles this by default\n",
    "        # Precompute protected‑shape mask (flat)\n",
    "        self.shape_mask = tf.Variable(\n",
    "            tf.tile(mask_flat[None, :], [N, 1]), trainable=False, dtype=tf.bool,\n",
    "            name=\"shape_mask\" # Add name for clarity in graph/debugging\n",
    "        )\n",
    "        # Represents the material currently present\n",
    "        self.stock = tf.Variable(\n",
    "            tf.ones([N, self.flat_dim], dtype=tf.bool),\n",
    "            trainable=False, name=\"stock\"\n",
    "        )\n",
    "\n",
    "        # Router positions (flat‑index), steps taken, done flags\n",
    "        self.pos = tf.Variable(tf.zeros([N], dtype=tf.int32), trainable=False, name=\"pos\")\n",
    "        self.steps = tf.Variable(tf.zeros([N], dtype=tf.int32), trainable=False, name=\"steps\")\n",
    "        self.done = tf.Variable(tf.zeros([N], dtype=tf.bool), trainable=False, name=\"done\")\n",
    "\n",
    "        # --- SHIFTS CALCULATION (Using Python ints) ---\n",
    "        G_py = grid_size # Use Python int G directly\n",
    "        def to_flat_py(dx, dy, dz):\n",
    "            # Pure Python integer calculation\n",
    "            return dx * G_py * G_py + dy * G_py + dz\n",
    "\n",
    "        moves = [(1,0,0),(-1,0,0),(0,1,0),(0,-1,0),(0,0,1),(0,0,-1)]\n",
    "        shifts_py = [to_flat_py(*m) for m in moves]\n",
    "        self.shifts = tf.constant(shifts_py, dtype=tf.int32, name=\"shifts\")  # [6]\n",
    "        # --- END SHIFTS CALCULATION ---\n",
    "\n",
    "\n",
    "    @tf.function # Compile reset logic\n",
    "    def reset(self):\n",
    "        # Reset stock (material), steps, done flags\n",
    "        self.stock.assign(tf.ones_like(self.stock, dtype=tf.bool))\n",
    "        self.steps.assign(tf.zeros_like(self.steps, dtype=tf.int32))\n",
    "        self.done.assign(tf.zeros_like(self.done, dtype=tf.bool))\n",
    "\n",
    "        # Pick a random *safe* start flat‑index for each env\n",
    "        safe_indices = tf.where(~self.shape_mask[0])[:, 0] # Get flat indices where mask is False [#safe]\n",
    "        num_safe = tf.shape(safe_indices)[0]\n",
    "\n",
    "        # Ensure we don't request more starting positions than available safe spots\n",
    "        num_envs_to_start = tf.minimum(self.N, num_safe)\n",
    "        tf.Assert(num_safe >= self.N, [\"Not enough safe starting positions available.\"])\n",
    "\n",
    "        shuffled_safe_indices = tf.random.shuffle(safe_indices)[:num_envs_to_start]\n",
    "\n",
    "        self.pos.assign(tf.cast(shuffled_safe_indices, tf.int32))\n",
    "        return self._get_obs()\n",
    "\n",
    "    @tf.function # Compile step logic\n",
    "    def step(self, actions):\n",
    "        # actions: [N] ints ∈ [0..5]\n",
    "        action_shifts = tf.gather(self.shifts, actions)     # [N]\n",
    "        new_pos = self.pos + action_shifts                 # [N], Potential new positions\n",
    "\n",
    "        # --- Boundary Checks ---\n",
    "        in_bounds = (new_pos >= 0) & (new_pos < self.flat_dim) # [N], boolean\n",
    "\n",
    "        # --- Clamp Indices for Safe Gathering ---\n",
    "        safe_new_pos = tf.clip_by_value(new_pos, 0, self.flat_dim - 1) # Shape [N]\n",
    "\n",
    "        # --- Gather State Information using tf.gather ---\n",
    "        shape_mask_at_old = tf.gather(self.shape_mask, self.pos, axis=1, batch_dims=1) # [N] bool\n",
    "        shape_mask_at_new = tf.gather(self.shape_mask, safe_new_pos, axis=1, batch_dims=1) # [N] bool\n",
    "        stock_at_old = tf.gather(self.stock, self.pos, axis=1, batch_dims=1) # [N] bool\n",
    "        stock_at_new = tf.gather(self.stock, safe_new_pos, axis=1, batch_dims=1) # [N] bool\n",
    "\n",
    "        # --- Collision and Removal Logic ---\n",
    "        hit_shape_or_oob = ~in_bounds | shape_mask_at_new # [N] bool\n",
    "        can_remove = ~hit_shape_or_oob & stock_at_new # [N] bool\n",
    "\n",
    "        # --- Calculate Rewards ---\n",
    "        reward = tf.where(hit_shape_or_oob, -5.0, 0.0) # [N] float32\n",
    "        reward = tf.where(can_remove, reward + 1.0, reward)\n",
    "        reward = reward - 0.1 # Constant step cost\n",
    "\n",
    "        # --- Update State ---\n",
    "        # 1. Update Stock\n",
    "        remove_indices = tf.where(can_remove) # Shape [num_removals, 1], dtype=int64\n",
    "        num_removals = tf.shape(remove_indices)[0]\n",
    "\n",
    "        def perform_update():\n",
    "            # Gather the actual valid new positions where removal occurred\n",
    "            # Squeeze remove_indices to 1D for gathering from 1D new_pos\n",
    "            pos_to_remove = tf.gather(new_pos, tf.squeeze(remove_indices, axis=1)) # Shape [num_removals]\n",
    "            # Create scatter indices [env_idx, pos_idx]\n",
    "            scatter_indices = tf.concat([\n",
    "                tf.cast(remove_indices, tf.int32), # Env indices [num_removals, 1]\n",
    "                tf.expand_dims(pos_to_remove, axis=1) # Pos indices [num_removals, 1]\n",
    "            ], axis=1) # Shape [num_removals, 2]\n",
    "            updates = tf.zeros(num_removals, dtype=tf.bool)\n",
    "            return tf.tensor_scatter_nd_update(self.stock, scatter_indices, updates)\n",
    "\n",
    "        maybe_updated_stock = tf.cond(\n",
    "            num_removals > 0,\n",
    "            true_fn=perform_update,\n",
    "            false_fn=lambda: self.stock # No-op, return current stock tensor\n",
    "        )\n",
    "        self.stock.assign(maybe_updated_stock)\n",
    "\n",
    "        # 2. Update Position: Move only if the move is valid\n",
    "        is_valid_move = ~hit_shape_or_oob # [N] bool\n",
    "        next_pos = tf.where(is_valid_move, new_pos, self.pos)\n",
    "        self.pos.assign(next_pos)\n",
    "\n",
    "        # 3. Update Steps and Done Flags\n",
    "        self.steps.assign_add(tf.ones_like(self.steps))\n",
    "        newly_done = (self.steps >= self.max_steps)\n",
    "        self.done.assign(self.done | newly_done)\n",
    "\n",
    "        return self._get_obs(), tf.cast(reward, tf.float32), self.done\n",
    "\n",
    "    @tf.function # Compile observation calculation\n",
    "    def _get_obs(self):\n",
    "        G_int = tf.cast(self.G, tf.int32)\n",
    "        z = self.pos % G_int\n",
    "        y = (self.pos // G_int) % G_int\n",
    "        x = self.pos // (G_int * G_int)\n",
    "        xyz = tf.stack([x, y, z], axis=1) # int32\n",
    "        center_coords = tf.constant([self.G/2 - 0.5, self.G/2 - 0.5, self.G/2 - 0.5], tf.float32)\n",
    "        center_tiled = tf.tile(center_coords[None, :], [self.N, 1]) # [N, 3] float32\n",
    "        obs = tf.concat([tf.cast(xyz, tf.float32), center_tiled], axis=1) # [N, 6] float32\n",
    "        return obs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Batched DQN Agent + Replay Buffer\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedReplayBuffer:\n",
    "    # Increased default capacity\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.cap = capacity\n",
    "        self.buf = []  # list of (S, A, R, S2, D) tuples, each element is a Tensor [N, ...]\n",
    "\n",
    "    def add_batch(self, S, A, R, S2, D):\n",
    "        if len(self.buf) >= self.cap:\n",
    "            self.buf.pop(0) # Remove oldest batch\n",
    "        self.buf.append((S, A, R, S2, D))\n",
    "\n",
    "    def sample(self, batch_size=128):\n",
    "        num_batches_in_buffer = len(self.buf)\n",
    "        if num_batches_in_buffer == 0: # Handle empty buffer case\n",
    "             return None\n",
    "\n",
    "        actual_batch_size = min(batch_size, num_batches_in_buffer)\n",
    "\n",
    "        if num_batches_in_buffer < batch_size:\n",
    "            indices = np.random.choice(num_batches_in_buffer, actual_batch_size, replace=True)\n",
    "            batch = [self.buf[i] for i in indices]\n",
    "        else:\n",
    "            batch = random.sample(self.buf, actual_batch_size)\n",
    "\n",
    "        if not batch:\n",
    "             return None\n",
    "\n",
    "        S_list, A_list, R_list, S2_list, D_list = zip(*batch)\n",
    "\n",
    "        S_sampled = tf.concat(S_list, axis=0)\n",
    "        A_sampled = tf.concat(A_list, axis=0)\n",
    "        R_sampled = tf.concat(R_list, axis=0)\n",
    "        S2_sampled = tf.concat(S2_list, axis=0)\n",
    "        D_sampled = tf.concat(D_list, axis=0)\n",
    "\n",
    "        return (S_sampled, A_sampled, R_sampled, S2_sampled, D_sampled)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "\n",
    "class BatchedDQNAgentTF:\n",
    "    def __init__(self, state_dim=6, action_dim=6,\n",
    "                 lr=1e-4, gamma=0.99, tau=0.005,  # Adjusted defaults\n",
    "                 n_envs=16): # Need n_envs for buffer size calculation\n",
    "        self.gamma, self.tau = gamma, tau\n",
    "        self.action_dim = action_dim\n",
    "        self.n_envs = n_envs # Store n_envs used in training\n",
    "\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(state_dim,)),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(action_dim, activation='linear') # Linear output for Q-values\n",
    "        ], name=\"Q_Model\")\n",
    "        self.target = tf.keras.models.clone_model(self.model)\n",
    "        self.target.set_weights(self.model.get_weights()) # Initial sync\n",
    "\n",
    "        self.opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.buffer = BatchedReplayBuffer()\n",
    "\n",
    "        logdir = f\"runs/batched_tf_{datetime.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "        self.writer = tf.summary.create_file_writer(logdir)\n",
    "        self.train_step_count = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"train_step_count\")\n",
    "\n",
    "    @tf.function # Compile training step\n",
    "    def train_step(self, S, A, R, S2, D):\n",
    "        # S, A, R, S2, D shapes: [B*N, state], [B*N], [B*N], [B*N, state], [B*N]\n",
    "\n",
    "        # Double DQN implementation\n",
    "        Q2_target = self.target(S2) # Q'(s', a) from target network\n",
    "        best_actions_next = tf.argmax(self.model(S2), axis=1, output_type=tf.int32) # a' = argmax_a Q(s', a) from online model\n",
    "\n",
    "        # Get Q'(s', a') using tf.gather_nd\n",
    "        action_indices = tf.stack([\n",
    "            tf.range(tf.shape(best_actions_next)[0], dtype=tf.int32), # Use int32 for indices\n",
    "            best_actions_next\n",
    "        ], axis=1)\n",
    "        Q2_best_target = tf.gather_nd(Q2_target, action_indices) # Q'(s', argmax_a Q(s', a))\n",
    "\n",
    "        # Compute TD target: y = R + gamma * Q'(s', a') * (1 - D)\n",
    "        target_Q = R + self.gamma * Q2_best_target * (1.0 - tf.cast(D, tf.float32))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_online = self.model(S) # Q(s, a) from online network\n",
    "            # Get Q(s, A) where A is the action actually taken\n",
    "            action_indices_taken = tf.stack([\n",
    "                 tf.range(tf.shape(A)[0], dtype=tf.int32), # Use int32 for indices\n",
    "                 A # Action A should already be int32\n",
    "            ], axis=1)\n",
    "            Q_online_taken = tf.gather_nd(Q_online, action_indices_taken) # Q(s, A)\n",
    "\n",
    "            # --- CORRECTED LOSS CALCULATION ---\n",
    "            # Compute loss (MSE using the Keras class)\n",
    "            loss = tf.keras.losses.MeanSquaredError()(target_Q, Q_online_taken)\n",
    "            # The MeanSquaredError class instance handles reduction over the batch.\n",
    "            # --- END CORRECTION ---\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # Polyak update target network weights\n",
    "        updated_target_weights = []\n",
    "        for w_online, w_target in zip(self.model.weights, self.target.weights):\n",
    "            updated_target_weights.append(self.tau * w_online + (1.0 - self.tau) * w_target)\n",
    "        self.target.set_weights(updated_target_weights)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Pass eps as a tf.Tensor to avoid retracing\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None, 6], dtype=tf.float32), # S\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float32)         # eps\n",
    "    ])\n",
    "    def act_batch(self, S, eps_tf):\n",
    "        # S shape: [N, state_dim]\n",
    "        batch_size = tf.shape(S)[0] # Should be N\n",
    "        q_values = self.model(S) # [N, action_dim]\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        random_actions = tf.random.uniform(shape=[batch_size], minval=0, maxval=self.action_dim, dtype=tf.int32)\n",
    "        greedy_actions = tf.argmax(q_values, axis=1, output_type=tf.int32) # [N]\n",
    "        # Use tf.random.uniform for comparison, ensuring graph mode compatibility\n",
    "        choose_random = tf.random.uniform(shape=[batch_size], minval=0.0, maxval=1.0) < eps_tf # Compare with tensor eps_tf\n",
    "        actions = tf.where(choose_random, random_actions, greedy_actions)\n",
    "\n",
    "        return actions # [N] int32\n",
    "\n",
    "    def remember_batch(self, S, A, R, S2, D):\n",
    "        self.buffer.add_batch(S, A, R, S2, D)\n",
    "\n",
    "    def learn(self, batch_size=128):\n",
    "        # Sample B batches -> B*N transitions\n",
    "        sampled_data = self.buffer.sample(batch_size)\n",
    "        if sampled_data is None:\n",
    "             # tf.print(\"DEBUG: Buffer too small, skipping learn\") # Add TF print for debug inside @tf.function if needed\n",
    "             return None # Not enough data\n",
    "\n",
    "        S_sampled, A_sampled, R_sampled, S2_sampled, D_sampled = sampled_data\n",
    "\n",
    "        # Perform one training step\n",
    "        loss = self.train_step(S_sampled, A_sampled, R_sampled, S2_sampled, D_sampled)\n",
    "\n",
    "        # Log loss and increment step counter\n",
    "        step = self.train_step_count.numpy() # Read step value for logging\n",
    "        with self.writer.as_default(step=step):\n",
    "             tf.summary.scalar(\"Train/Loss\", loss) # Log the scalar loss tensor\n",
    "        self.train_step_count.assign_add(1) # Increment TF variable\n",
    "        return loss.numpy() # Return loss value for potential printing\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Training Loop\n",
    "# -----------------------------------------------------------------------------\n",
    "# import time # Ensure time is imported (should be done in main block)\n",
    "\n",
    "def train_gpu_batched(\n",
    "        grid_size=32, max_steps=400,       # Env params\n",
    "        n_envs=64, episodes=1000,          # Batching/Loop params\n",
    "        buffer_capacity=100000,             # Buffer param (passed to agent/buffer)\n",
    "        learn_batch_size=64, learn_freq=4, # Learning control (learn every X env steps)\n",
    "        eps0=1.0, eps_end=0.05, eps_steps=200000, # Epsilon decay over agent steps\n",
    "        gamma=0.99, lr=1e-4, tau=0.005,     # Agent HPs\n",
    "        log_every=20, render_every=100      # Logging/Rendering frequency (episodes)\n",
    "        ):\n",
    "\n",
    "    print(f\"Starting training with N_Envs={n_envs}, Grid={grid_size}, MaxSteps={max_steps}\")\n",
    "    env = BatchedSculpt3DEnvTF(grid_size, max_steps, n_envs)\n",
    "    print(f\"DEBUG: Environment initialized.\") # DEBUG PRINT\n",
    "    agent = BatchedDQNAgentTF(state_dim=6, action_dim=6, lr=lr, gamma=gamma, tau=tau, n_envs=n_envs)\n",
    "    print(f\"DEBUG: Agent initialized.\") # DEBUG PRINT\n",
    "    agent.buffer.cap = buffer_capacity\n",
    "\n",
    "    total_steps_taken = 0 # Tracks total interactions (N * parallel steps)\n",
    "    eps = eps0 # Python float for calculation\n",
    "\n",
    "    episode_rewards_history = [] # Store average reward per episode batch\n",
    "    episode_lengths_history = [] # Store average length per episode batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    for ep in range(1, episodes + 1):\n",
    "        #print(f\"\\nDEBUG: Starting Episode Batch {ep}/{episodes}\") # DEBUG PRINT\n",
    "        ep_start_time = time.time() # Time each episode batch\n",
    "\n",
    "        obs = env.reset()                     # [N, 6]\n",
    "        #print(f\"DEBUG: Episode Batch {ep} - env.reset() completed.\") # DEBUG PRINT\n",
    "        done = tf.zeros([n_envs], tf.bool)    # [N]\n",
    "        ep_rewards = tf.zeros([n_envs], tf.float32) # Track rewards per env within this episode batch\n",
    "        ep_steps = tf.zeros([n_envs], tf.int32)     # Track steps per env\n",
    "\n",
    "        current_ep_step = 0 # Counter for steps within the current parallel episode batch\n",
    "        inner_loop_start_time = time.time() # Time the inner loop\n",
    "\n",
    "        while not tf.reduce_all(done):\n",
    "            step_start_time = time.time() # Time each parallel step\n",
    "\n",
    "            # Epsilon calculation based on total agent interactions (using Python float)\n",
    "            eps = max(eps_end, eps0 - (eps0 - eps_end) * (total_steps_taken / eps_steps))\n",
    "\n",
    "            # --- FIX FOR RETRACING ---\n",
    "            # Convert Python float eps to a Tensor before passing to act_batch\n",
    "            eps_tf = tf.constant(eps, dtype=tf.float32)\n",
    "            # Choose action batch using epsilon-greedy\n",
    "            A = agent.act_batch(obs, eps_tf) # Pass Tensor eps_tf\n",
    "            # --- END FIX ---\n",
    "\n",
    "            # Step the environment batch\n",
    "            S2, R, next_done = env.step(A) # [N, 6], [N], [N]\n",
    "\n",
    "            # Store the batch of transitions in the replay buffer\n",
    "            agent.remember_batch(obs, A, R, S2, next_done)\n",
    "\n",
    "            # Update rewards and steps for environments that were active this step\n",
    "            active_mask = ~done\n",
    "            ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "            ep_steps += tf.cast(active_mask, tf.int32)\n",
    "\n",
    "            # Update observations and done flags for the next iteration\n",
    "            obs = S2\n",
    "            done = next_done # Use the new done flags from the env step\n",
    "\n",
    "            # Increment counters\n",
    "            total_steps_taken += n_envs\n",
    "            current_ep_step += 1\n",
    "\n",
    "            # --- SIMPLIFIED LEARNING FREQUENCY LOGIC ---\n",
    "            learn_loss = None\n",
    "            if current_ep_step > 0 and current_ep_step % learn_freq == 0:\n",
    "                learn_start_time = time.time() # Time learning step\n",
    "                learn_loss = agent.learn(learn_batch_size)\n",
    "                learn_duration = time.time() - learn_start_time\n",
    "                # Optional: Print learn duration if it seems slow\n",
    "                # if learn_duration > 0.5: # If learn takes > 0.5 sec\n",
    "                #    print(f\"DEBUG: Ep {ep}, Step {current_ep_step} - Learn duration: {learn_duration:.3f}s\")\n",
    "\n",
    "            # --- DEBUG PRINT inside inner loop ---\n",
    "            #step_duration = time.time() - step_start_time\n",
    "            #if current_ep_step % 100 == 0: # Print every 100 steps\n",
    "                 # Display learn_loss which might be None if learning didn't happen this step\n",
    "                 #print(f\"DEBUG: Ep {ep}, Step {current_ep_step}/{max_steps} | Step Time: {step_duration:.4f}s | Learn Loss: {learn_loss}\")\n",
    "            # --- END DEBUG PRINT ---\n",
    "\n",
    "\n",
    "        # --- Episode Batch Finished ---\n",
    "        inner_loop_duration = time.time() - inner_loop_start_time\n",
    "        #print(f\"DEBUG: Episode Batch {ep} finished inner loop ({current_ep_step} steps) in {inner_loop_duration:.2f}s.\") # DEBUG PRINT\n",
    "\n",
    "        avg_reward_batch = tf.reduce_mean(ep_rewards).numpy()\n",
    "        avg_steps_batch = tf.reduce_mean(tf.cast(ep_steps, tf.float32)).numpy()\n",
    "        episode_rewards_history.append(avg_reward_batch)\n",
    "        episode_lengths_history.append(avg_steps_batch)\n",
    "\n",
    "        # Log episode statistics\n",
    "        if ep % log_every == 0 or ep == 1: # Log on first episode too\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # Calculate stats over the last 'log_every' episode batches\n",
    "            avg_r = np.mean(episode_rewards_history[-log_every:]) if episode_rewards_history else 0.0\n",
    "            avg_l = np.mean(episode_lengths_history[-log_every:]) if episode_lengths_history else 0.0\n",
    "            print(f\"Ep {ep}/{episodes} | Avg R: {avg_r:.2f} | Avg Len: {avg_l:.1f} | Eps: {eps:.3f} | Steps: {total_steps_taken} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "            # Log to TensorBoard (using agent's writer and agent step count)\n",
    "            agent_step = agent.train_step_count.numpy()\n",
    "            with agent.writer.as_default(step=agent_step):\n",
    "                 tf.summary.scalar(\"Episode/AvgReward\", avg_r)\n",
    "                 tf.summary.scalar(\"Episode/AvgLength\", avg_l)\n",
    "                 tf.summary.scalar(\"Params/Epsilon\", eps) # Log the python float eps value\n",
    "\n",
    "\n",
    "        # --- Rendering ---\n",
    "        if ep % render_every == 0:\n",
    "            print(f\"--- Rendering Placeholder at episode {ep} ---\")\n",
    "            # Find the environment index with the highest reward in the completed batch\n",
    "            best_env_index = int(tf.argmax(ep_rewards).numpy())\n",
    "            best_reward = ep_rewards[best_env_index].numpy()\n",
    "            print(f\"Rendering requires path reconstruction (currently disabled for performance).\")\n",
    "            print(f\"Best env in last batch: Index {best_env_index}, Reward {best_reward:.2f}\")\n",
    "            # --- RENDER CODE NEEDS REWORK ---\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    agent.writer.close()\n",
    "    print(f\"Training finished. Total steps: {total_steps_taken}\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure necessary libraries are imported at the top\n",
    "    import os\n",
    "    # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Keep commented out to see TF logs\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "    import time\n",
    "\n",
    "    # Set random seeds for reproducibility (optional)\n",
    "    # seed = 42\n",
    "    # np.random.seed(seed)\n",
    "    # random.seed(seed)\n",
    "    # tf.random.set_seed(seed)\n",
    "\n",
    "    # --- Parameters for training ---\n",
    "    # Consider reducing n_envs significantly first if suspecting OOM or slowness\n",
    "    N_ENVS_TO_RUN = 128 # Original: 128. Try 32 or 16 for debugging.\n",
    "    GRID_SIZE_TO_RUN = 32 # Original: 32. Try 20 for debugging.\n",
    "\n",
    "    agent = train_gpu_batched(\n",
    "        grid_size=GRID_SIZE_TO_RUN,     # Use variable\n",
    "        max_steps=500,\n",
    "        n_envs=N_ENVS_TO_RUN,           # Use variable\n",
    "        episodes=1000,\n",
    "        buffer_capacity=200000,\n",
    "        learn_batch_size=64,\n",
    "        learn_freq=4,\n",
    "        eps_steps=500000,\n",
    "        eps_end=0.02,\n",
    "        lr=5e-5,\n",
    "        tau=0.005,\n",
    "        log_every=20,\n",
    "        render_every=500\n",
    "    )\n",
    "    # Optional: Save the trained model\n",
    "    # save_path = \"sculpt_dqn_model.keras\"\n",
    "    # agent.model.save(save_path)\n",
    "    # print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% imports (ensure these are imported in your notebook)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Assume BatchedSculpt3DEnvTF and BatchedDQNAgentTF classes are defined\n",
    "# Assume trained_agent, GRID_SIZE_TO_RUN, MAX_STEPS_TRAIN are available\n",
    "# from the previous training cell.\n",
    "\n",
    "# %% Evaluation Function Definition\n",
    "\n",
    "def evaluate_agent_stats(agent, grid_size, max_steps, num_test_episodes=10, render_env_index=0):\n",
    "    \"\"\"\n",
    "    Evaluates a trained agent greedily, calculates performance statistics,\n",
    "    and renders the final state of one specified environment.\n",
    "\n",
    "    Args:\n",
    "        agent: The trained BatchedDQNAgentTF object.\n",
    "        grid_size: The grid size used during training.\n",
    "        max_steps: The max steps per episode used during training.\n",
    "        num_test_episodes: How many episodes to run for evaluation.\n",
    "        render_env_index: The index of the environment (0 to num_test_episodes-1)\n",
    "                           whose final state will be rendered.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Agent Evaluation ---\")\n",
    "    print(f\"Running {num_test_episodes} evaluation episodes...\")\n",
    "\n",
    "    # Create a test environment batch\n",
    "    test_env = BatchedSculpt3DEnvTF(grid_size=grid_size, max_steps=max_steps, n_envs=num_test_episodes)\n",
    "\n",
    "    # --- Get Initial State Information ---\n",
    "    # We need the shape mask to know what *can* be carved.\n",
    "    # Since reset() sets stock to all True, initial carvable material is just ~shape_mask.\n",
    "    initial_shape_mask_flat_gpu = test_env.shape_mask[0] # Mask is same for all envs\n",
    "    initial_shape_mask_flat_np = initial_shape_mask_flat_gpu.numpy()\n",
    "    # Calculate initial carvable material count (where stock is True and mask is False)\n",
    "    initial_carvable_mask_flat = ~initial_shape_mask_flat_np # Initially stock is True everywhere\n",
    "    initial_carvable_count = np.sum(initial_carvable_mask_flat)\n",
    "    print(f\"Initial number of carvable voxels: {initial_carvable_count}\")\n",
    "    if initial_carvable_count == 0:\n",
    "        print(\"Warning: No carvable material initially defined by the shape mask.\")\n",
    "\n",
    "    # --- Run Evaluation Episodes ---\n",
    "    all_ep_rewards = []\n",
    "    all_ep_lengths = []\n",
    "    all_ep_removed_counts = []\n",
    "    all_ep_removal_percentages = []\n",
    "\n",
    "    # References to final state variables\n",
    "    final_stock_variable_test = test_env.stock\n",
    "    final_shape_mask_variable_test = test_env.shape_mask # Although constant, get reference\n",
    "\n",
    "    for ep in range(num_test_episodes):\n",
    "        obs = test_env.reset()\n",
    "        done = tf.zeros([num_test_episodes], tf.bool)\n",
    "        ep_rewards = tf.zeros([num_test_episodes], tf.float32)\n",
    "        ep_steps = tf.zeros([num_test_episodes], tf.int32)\n",
    "        current_ep_step = 0\n",
    "\n",
    "        while not tf.reduce_all(done):\n",
    "            # Act greedily (epsilon = 0)\n",
    "            eps_tf_zero = tf.constant(0.0, dtype=tf.float32)\n",
    "            A = agent.act_batch(obs, eps_tf_zero)\n",
    "            S2, R, next_done = test_env.step(A)\n",
    "\n",
    "            active_mask = ~done\n",
    "            ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "            ep_steps += tf.cast(active_mask, tf.int32)\n",
    "\n",
    "            obs = S2\n",
    "            done = next_done\n",
    "            current_ep_step += 1\n",
    "\n",
    "        # --- Calculate Stats for this Batch ---\n",
    "        final_stock_batch_np = final_stock_variable_test.numpy() # Get final stock for all envs in batch\n",
    "        batch_rewards = ep_rewards.numpy()\n",
    "        batch_lengths = ep_steps.numpy()\n",
    "\n",
    "        all_ep_rewards.extend(batch_rewards.tolist())\n",
    "        all_ep_lengths.extend(batch_lengths.tolist())\n",
    "\n",
    "        # Calculate removal stats per environment\n",
    "        for i in range(num_test_episodes):\n",
    "            final_stock_flat_np = final_stock_batch_np[i]\n",
    "            # Material that was initially carvable and is now removed (~final_stock)\n",
    "            removed_mask = initial_carvable_mask_flat & (~final_stock_flat_np)\n",
    "            removed_count = np.sum(removed_mask)\n",
    "            all_ep_removed_counts.append(removed_count)\n",
    "\n",
    "            if initial_carvable_count > 0:\n",
    "                removal_percentage = (removed_count / initial_carvable_count) * 100.0\n",
    "                all_ep_removal_percentages.append(removal_percentage)\n",
    "            else:\n",
    "                all_ep_removal_percentages.append(0.0) # Avoid division by zero\n",
    "\n",
    "        print(f\"Eval Episode {ep+1}/{num_test_episodes} finished. Steps: {current_ep_step}\") # Removed rewards print here\n",
    "\n",
    "    # --- Aggregate and Print Statistics ---\n",
    "    if not all_ep_rewards:\n",
    "        print(\"\\nNo evaluation episodes were run.\")\n",
    "        return\n",
    "\n",
    "    avg_reward = np.mean(all_ep_rewards)\n",
    "    avg_length = np.mean(all_ep_lengths)\n",
    "    avg_removed_count = np.mean(all_ep_removed_counts)\n",
    "    avg_removal_percentage = np.mean(all_ep_removal_percentages)\n",
    "    std_reward = np.std(all_ep_rewards)\n",
    "    std_removal_percentage = np.std(all_ep_removal_percentages)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results ({num_test_episodes} episodes) ---\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f} (+/- {std_reward:.2f})\")\n",
    "    print(f\"Average Length: {avg_length:.1f}\")\n",
    "    print(f\"Average Carvable Voxels Removed: {avg_removed_count:.1f} / {initial_carvable_count}\")\n",
    "    print(f\"Average Removal Percentage: {avg_removal_percentage:.2f}% (+/- {std_removal_percentage:.2f}%)\")\n",
    "\n",
    "    # --- Render Final State ---\n",
    "    print(f\"\\n--- Rendering final state for Eval Env Index: {render_env_index} ---\")\n",
    "    if render_env_index < 0 or render_env_index >= num_test_episodes:\n",
    "         print(f\"Error: render_env_index ({render_env_index}) out of bounds for {num_test_episodes} test environments.\")\n",
    "         return\n",
    "\n",
    "    try:\n",
    "        # Get final stock and shape mask for the specific env index from the *last* batch run\n",
    "        final_stock_flat_np = final_stock_batch_np[render_env_index]\n",
    "        # Mask is constant, can re-use from initial calculation\n",
    "        # final_shape_mask_flat_np = final_shape_mask_variable_test[0].numpy()\n",
    "\n",
    "        # Reshape to 3D grid\n",
    "        G = grid_size\n",
    "        final_stock_3d = final_stock_flat_np.reshape((G, G, G))\n",
    "        final_shape_mask_3d = initial_shape_mask_flat_np.reshape((G, G, G))\n",
    "\n",
    "        # Create boolean arrays for plotting:\n",
    "        shape_to_plot = final_shape_mask_3d\n",
    "        removed_material_to_plot = ~final_stock_3d & ~final_shape_mask_3d\n",
    "\n",
    "        # --- Matplotlib Visualization ---\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        x, y, z = np.indices(np.array(shape_to_plot.shape) + 1) # Edges\n",
    "\n",
    "        # Plot the target shape (slightly transparent blue)\n",
    "        ax.voxels(x, y, z, shape_to_plot, facecolors='blue', alpha=0.15, edgecolor=None)\n",
    "        # Plot the removed material (more opaque red)\n",
    "        ax.voxels(x, y, z, removed_material_to_plot, facecolors='red', alpha=0.5, edgecolor=None)\n",
    "\n",
    "        # Use the reward from the specific rendered env\n",
    "        rendered_env_reward = all_ep_rewards[render_env_index]\n",
    "        rendered_env_perc = all_ep_removal_percentages[render_env_index]\n",
    "        title_reward = f\"{rendered_env_reward:.2f}\"\n",
    "        ax.set_title(f\"Eval Render Env #{render_env_index} (R={title_reward}, Removed={rendered_env_perc:.1f}%)\")\n",
    "\n",
    "        ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "        ax.set_xlim(0, G); ax.set_ylim(0, G); ax.set_zlim(0, G)\n",
    "        ax.set_aspect('auto')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save figure\n",
    "        render_dir = \"renders_eval\"\n",
    "        if not os.path.exists(render_dir):\n",
    "            os.makedirs(render_dir)\n",
    "        save_path = os.path.join(render_dir, f\"eval_render_env_{render_env_index}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved evaluation render to {save_path}\")\n",
    "        plt.close(fig) # Close figure\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during rendering: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# %% Run Evaluation (Example Call)\n",
    "# Make sure trained_agent, GRID_SIZE_TO_RUN, and MAX_STEPS_TRAIN are defined\n",
    "# from your training process before running this cell.\n",
    "\n",
    "NUM_EVAL_EPISODES = 10 # Number of episodes to average over for evaluation\n",
    "RENDER_INDEX = 0       # Which episode's final state to render (0 to NUM_EVAL_EPISODES-1)\n",
    "\n",
    "if 'trained_agent' in locals() or 'trained_agent' in globals():\n",
    "     evaluate_agent_stats(\n",
    "         agent=trained_agent,\n",
    "         grid_size=GRID_SIZE_TO_RUN,\n",
    "         max_steps=MAX_STEPS_TRAIN,\n",
    "         num_test_episodes=NUM_EVAL_EPISODES,\n",
    "         render_env_index=RENDER_INDEX\n",
    "     )\n",
    "else:\n",
    "     print(\"Variable 'trained_agent' not found. Please train the agent first.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
