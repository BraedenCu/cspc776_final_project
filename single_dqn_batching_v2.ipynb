{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 # CURRENT\n",
    "\"\"\"\n",
    "Fully‑GPU Batched Sculpt3DEnv + DQN Agent (Hybrid State & Noisy Nets)\n",
    "\n",
    "• State Representation: Hybrid - CNN processes Stock/Mask grids, concatenates\n",
    "                       with normalized XYZ coordinates.\n",
    "• Agent Network: Uses a 3D CNN feature extractor + Dense head with Noisy Layers.\n",
    "• Exploration: Uses Noisy Networks instead of epsilon-greedy.\n",
    "• N envs stepped in parallel during training.\n",
    "• Batched replay buffer insertion.\n",
    "• Includes periodic evaluation during training and final evaluation/rendering.\n",
    "• Plots carving performance trend at the end of training.\n",
    "\n",
    "NOTE: Monitor memory usage. Tune hyperparameters (LR, network, etc.).\n",
    "      Periodic evaluation adds runtime overhead.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# Suppress TensorFlow INFO/WARNING messages\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import time\n",
    "import math # For noisy layer initialization\n",
    "\n",
    "# --- Configure GPU Memory Growth ---\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs configured with Memory Growth.\")\n",
    "  except RuntimeError as e:\n",
    "    print(f\"Memory growth error: {e}\") # Might happen if GPU already initialized\n",
    "else:\n",
    "    print(\"No GPU detected by TensorFlow.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Batched Sculpt3DEnvTF (Hybrid Observation)\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedSculpt3DEnvTF:\n",
    "    def __init__(self, grid_size=16, max_steps=200, n_envs=16):\n",
    "        G, N = grid_size, n_envs\n",
    "        if N <= 0: raise ValueError(\"n_envs must be positive.\")\n",
    "        self.G, self.N, self.max_steps = G, N, max_steps\n",
    "        self.flat_dim = G*G*G\n",
    "        self.grid_obs_shape = (G, G, G, 2) # Channels: Stock, ShapeMask\n",
    "        self.coord_obs_shape = (3,)        # Channels: X, Y, Z (normalized)\n",
    "\n",
    "        coords_range = tf.range(G, dtype=tf.float32)\n",
    "        coords = tf.stack(tf.meshgrid(coords_range, coords_range, coords_range, indexing='ij'), axis=-1)\n",
    "        center = tf.constant([G/2 - 0.5, G/2 - 0.5, G/2 - 0.5], tf.float32)\n",
    "        dist2 = tf.reduce_sum(tf.square(coords - center), axis=-1)\n",
    "        radius_sq = tf.square(tf.cast(G // 2 - 1, tf.float32))\n",
    "        mask3d = dist2 <= radius_sq\n",
    "        mask_flat = tf.reshape(mask3d, [-1])\n",
    "\n",
    "        self.shape_mask = tf.Variable(tf.tile(mask_flat[None, :], [N, 1]), trainable=False, dtype=tf.bool, name=\"shape_mask\")\n",
    "        self.stock = tf.Variable(tf.ones([N, self.flat_dim], dtype=tf.bool), trainable=False, name=\"stock\")\n",
    "        self.pos = tf.Variable(tf.zeros([N], dtype=tf.int32), trainable=False, name=\"pos\")\n",
    "        self.steps = tf.Variable(tf.zeros([N], dtype=tf.int32), trainable=False, name=\"steps\")\n",
    "        self.done = tf.Variable(tf.zeros([N], dtype=tf.bool), trainable=False, name=\"done\")\n",
    "\n",
    "        G_py = grid_size\n",
    "        def to_flat_py(dx, dy, dz): return dx*G_py*G_py + dy*G_py + dz\n",
    "        moves = [(1,0,0),(-1,0,0),(0,1,0),(0,-1,0),(0,0,1),(0,0,-1)]\n",
    "        shifts_py = [to_flat_py(*m) for m in moves]\n",
    "        self.shifts = tf.constant(shifts_py, dtype=tf.int32, name=\"shifts\")\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self):\n",
    "        self.stock.assign(tf.ones_like(self.stock))\n",
    "        self.steps.assign(tf.zeros_like(self.steps))\n",
    "        self.done.assign(tf.zeros_like(self.done))\n",
    "        safe_indices = tf.where(~self.shape_mask[0])[:, 0]\n",
    "        num_safe = tf.shape(safe_indices)[0]\n",
    "        tf.Assert(num_safe >= self.N, [\"Not enough safe starting positions available.\"])\n",
    "        shuffled_safe_indices = tf.random.shuffle(safe_indices)[:self.N]\n",
    "        self.pos.assign(tf.cast(shuffled_safe_indices, tf.int32))\n",
    "        return self._get_obs()\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, actions):\n",
    "        action_shifts = tf.gather(self.shifts, actions)\n",
    "        new_pos = self.pos + action_shifts\n",
    "        in_bounds = (new_pos >= 0) & (new_pos < self.flat_dim)\n",
    "        safe_new_pos = tf.clip_by_value(new_pos, 0, self.flat_dim - 1)\n",
    "        shape_mask_at_new = tf.gather(self.shape_mask, safe_new_pos, axis=1, batch_dims=1)\n",
    "        stock_at_new = tf.gather(self.stock, safe_new_pos, axis=1, batch_dims=1)\n",
    "        hit_shape_or_oob = ~in_bounds | shape_mask_at_new\n",
    "        can_remove = ~hit_shape_or_oob & stock_at_new\n",
    "        reward = tf.where(hit_shape_or_oob, -5.0, 0.0)\n",
    "        reward = tf.where(can_remove, reward + 1.0, reward)\n",
    "        reward = reward - 0.1\n",
    "        remove_indices = tf.where(can_remove)\n",
    "        num_removals = tf.shape(remove_indices)[0]\n",
    "        def perform_update():\n",
    "            pos_to_remove = tf.gather(new_pos, tf.squeeze(remove_indices, axis=1))\n",
    "            scatter_indices = tf.concat([tf.cast(remove_indices, tf.int32), tf.expand_dims(pos_to_remove, axis=1)], axis=1)\n",
    "            updates = tf.zeros(num_removals, dtype=tf.bool)\n",
    "            return tf.tensor_scatter_nd_update(self.stock, scatter_indices, updates)\n",
    "        maybe_updated_stock = tf.cond(num_removals > 0, true_fn=perform_update, false_fn=lambda: self.stock)\n",
    "        self.stock.assign(maybe_updated_stock)\n",
    "        is_valid_move = ~hit_shape_or_oob\n",
    "        next_pos = tf.where(is_valid_move, new_pos, self.pos)\n",
    "        self.pos.assign(next_pos)\n",
    "        self.steps.assign_add(tf.ones_like(self.steps))\n",
    "        newly_done = (self.steps >= self.max_steps)\n",
    "        self.done.assign(self.done | newly_done)\n",
    "        next_obs = self._get_obs()\n",
    "        return next_obs, tf.cast(reward, tf.float32), self.done\n",
    "\n",
    "    @tf.function\n",
    "    def _get_obs(self):\n",
    "        G = self.G; N = self.N\n",
    "        stock_grid = tf.reshape(self.stock, [N, G, G, G])\n",
    "        shape_mask_grid = tf.reshape(self.shape_mask, [N, G, G, G])\n",
    "        stock_float = tf.cast(stock_grid, tf.float32)\n",
    "        shape_mask_float = tf.cast(shape_mask_grid, tf.float32)\n",
    "        grid_obs = tf.stack([stock_float, shape_mask_float], axis=-1)\n",
    "        g_tf = tf.constant(G, dtype=tf.int32)\n",
    "        z = self.pos % g_tf\n",
    "        y = (self.pos // g_tf) % g_tf\n",
    "        x = self.pos // (g_tf * g_tf)\n",
    "        g_minus_1_float = tf.cast(tf.maximum(1, G - 1), tf.float32)\n",
    "        x_norm = tf.cast(x, tf.float32) / g_minus_1_float\n",
    "        y_norm = tf.cast(y, tf.float32) / g_minus_1_float\n",
    "        z_norm = tf.cast(z, tf.float32) / g_minus_1_float\n",
    "        coord_obs = tf.stack([x_norm, y_norm, z_norm], axis=-1)\n",
    "        return (grid_obs, coord_obs)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Replay Buffer (Handling Tuple State)\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.cap = capacity; self.buf = []\n",
    "    def add_batch(self, S_tuple, A, R, S2_tuple, D):\n",
    "        if len(self.buf) >= self.cap: self.buf.pop(0)\n",
    "        self.buf.append((S_tuple, A, R, S2_tuple, D))\n",
    "    def sample(self, batch_size=32):\n",
    "        num = len(self.buf)\n",
    "        if num < batch_size: return None\n",
    "        indices = random.sample(range(num), batch_size)\n",
    "        batch = [self.buf[i] for i in indices]\n",
    "        S_tuple_list, A_list, R_list, S2_tuple_list, D_list = zip(*batch)\n",
    "        S_grid_list, S_coord_list = zip(*S_tuple_list)\n",
    "        S2_grid_list, S2_coord_list = zip(*S2_tuple_list)\n",
    "        return (tf.concat(S_grid_list, axis=0), tf.concat(S_coord_list, axis=0),\n",
    "                tf.concat(A_list, axis=0), tf.concat(R_list, axis=0),\n",
    "                tf.concat(S2_grid_list, axis=0), tf.concat(S2_coord_list, axis=0),\n",
    "                tf.concat(D_list, axis=0))\n",
    "    def __len__(self): return len(self.buf)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Noisy Dense Layer (Factorized Gaussian Noise)\n",
    "# -----------------------------------------------------------------------------\n",
    "class NoisyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.sigma0 = 0.5\n",
    "    def build(self, input_shape):\n",
    "        in_features = input_shape[-1]; out_features = self.units; dtype = tf.float32\n",
    "        sigma_init_val = self.sigma0 / math.sqrt(float(in_features))\n",
    "        sigma_init = tf.constant_initializer(sigma_init_val)\n",
    "        self.kernel_mean = self.add_weight(name=\"kernel_mean\", shape=(in_features, out_features), initializer=\"he_uniform\", trainable=True, dtype=dtype)\n",
    "        self.bias_mean = self.add_weight(name=\"bias_mean\", shape=(out_features,), initializer=\"zeros\", trainable=True, dtype=dtype)\n",
    "        self.kernel_sigma = self.add_weight(name=\"kernel_sigma\", shape=(in_features, out_features), initializer=sigma_init, trainable=True, dtype=dtype)\n",
    "        self.bias_sigma = self.add_weight(name=\"bias_sigma\", shape=(out_features,), initializer=sigma_init, trainable=True, dtype=dtype)\n",
    "        super().build(input_shape)\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None: training = tf.keras.backend.learning_phase()\n",
    "        if training:\n",
    "            noise_in = self._get_noise(tf.shape(inputs)[-1])\n",
    "            noise_out = self._get_noise(self.units)\n",
    "            kernel_noise = tf.tensordot(tf.expand_dims(noise_in, -1), tf.expand_dims(noise_out, 0), axes=1)\n",
    "            bias_noise = noise_out\n",
    "            kernel = self.kernel_mean + self.kernel_sigma * kernel_noise\n",
    "            bias = self.bias_mean + self.bias_sigma * bias_noise\n",
    "        else:\n",
    "            kernel = self.kernel_mean; bias = self.bias_mean\n",
    "        output = tf.matmul(inputs, kernel) + bias\n",
    "        if self.activation is not None: output = self.activation(output)\n",
    "        return output\n",
    "    def _get_noise(self, num_elements):\n",
    "        noise = tf.random.normal(shape=[num_elements]) # Use rank-1 shape\n",
    "        return tf.sign(noise) * tf.sqrt(tf.abs(noise))\n",
    "    def compute_output_shape(self, input_shape): return tuple(input_shape[:-1]) + (self.units,)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Batched DQN Agent (Hybrid State CNN + Noisy Nets)\n",
    "# -----------------------------------------------------------------------------\n",
    "class BatchedDQNAgentTF:\n",
    "    def __init__(self, grid_shape, coord_shape, action_dim=6,\n",
    "                 lr=1e-4, gamma=0.99, tau=0.005):\n",
    "        self.grid_shape = grid_shape; self.coord_shape = coord_shape\n",
    "        self.action_dim = action_dim; self.gamma = gamma; self.tau = tau\n",
    "        def build_hybrid_noisy_model():\n",
    "            grid_input = tf.keras.layers.Input(shape=self.grid_shape, name=\"grid_input\")\n",
    "            coord_input = tf.keras.layers.Input(shape=self.coord_shape, name=\"coord_input\")\n",
    "            x_cnn = tf.keras.layers.Conv3D(filters=32, kernel_size=5, strides=2, activation='relu', padding='same')(grid_input)\n",
    "            x_cnn = tf.keras.layers.Conv3D(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')(x_cnn)\n",
    "            x_cnn = tf.keras.layers.Conv3D(filters=64, kernel_size=3, strides=1, activation='relu', padding='same')(x_cnn)\n",
    "            cnn_features = tf.keras.layers.Flatten()(x_cnn)\n",
    "            concat_features = tf.keras.layers.Concatenate()([cnn_features, coord_input])\n",
    "            x = NoisyDense(512, activation='relu')(concat_features)\n",
    "            outputs = NoisyDense(action_dim, activation='linear')(x)\n",
    "            return tf.keras.Model(inputs=[grid_input, coord_input], outputs=outputs)\n",
    "        self.model = build_hybrid_noisy_model()\n",
    "        self.target = build_hybrid_noisy_model()\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "        self.opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.buffer = BatchedReplayBuffer()\n",
    "        logdir = f\"runs/hybrid_noisy_dqn_{datetime.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "        self.writer = tf.summary.create_file_writer(logdir)\n",
    "        self.train_step_count = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, S_grid, S_coord, A, R, S2_grid, S2_coord, D):\n",
    "        Q2_target = self.target([S2_grid, S2_coord], training=True)\n",
    "        Q2_online = self.model([S2_grid, S2_coord], training=True)\n",
    "        best_actions_next = tf.argmax(Q2_online, axis=1, output_type=tf.int32)\n",
    "        action_indices = tf.stack([tf.range(tf.shape(best_actions_next)[0], dtype=tf.int32), best_actions_next], axis=1)\n",
    "        Q2_best_target = tf.gather_nd(Q2_target, action_indices)\n",
    "        target_Q = R + self.gamma * Q2_best_target * (1.0 - tf.cast(D, tf.float32))\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_online = self.model([S_grid, S_coord], training=True)\n",
    "            action_indices_taken = tf.stack([tf.range(tf.shape(A)[0], dtype=tf.int32), A], axis=1)\n",
    "            Q_online_taken = tf.gather_nd(Q_online, action_indices_taken)\n",
    "            loss = tf.keras.losses.MeanSquaredError()(target_Q, Q_online_taken)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        updated_target_weights = []\n",
    "        for w_online, w_target in zip(self.model.weights, self.target.weights):\n",
    "            updated_target_weights.append(self.tau * w_online + (1.0 - self.tau) * w_target)\n",
    "        self.target.set_weights(updated_target_weights)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def act_batch(self, S_tuple, deterministic=False):\n",
    "        q_values = self.model(S_tuple, training=not deterministic)\n",
    "        actions = tf.argmax(q_values, axis=1, output_type=tf.int32)\n",
    "        return actions\n",
    "\n",
    "    def remember_batch(self, S_tuple, A, R, S2_tuple, D):\n",
    "        self.buffer.add_batch(S_tuple, A, R, S2_tuple, D)\n",
    "\n",
    "    def learn(self, batch_size=32):\n",
    "        if len(self.buffer) < batch_size: return None\n",
    "        sampled_data = self.buffer.sample(batch_size)\n",
    "        if sampled_data is None: return None\n",
    "        S_grid_s, S_coord_s, A_s, R_s, S2_grid_s, S2_coord_s, D_s = sampled_data\n",
    "        loss = self.train_step(S_grid_s, S_coord_s, A_s, R_s, S2_grid_s, S2_coord_s, D_s)\n",
    "        step = self.train_step_count.numpy()\n",
    "        with self.writer.as_default(step=step): tf.summary.scalar(\"Train/Loss\", loss)\n",
    "        self.train_step_count.assign_add(1)\n",
    "        return loss.numpy()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Evaluation Function (Moved Before Training Loop & Returns Stats)\n",
    "# -----------------------------------------------------------------------------\n",
    "def evaluate_agent_performance(agent, grid_size, max_steps, num_eval_episodes=10, render=True, render_env_index=0):\n",
    "    \"\"\"\n",
    "    Evaluates a trained agent deterministically, calculates performance statistics,\n",
    "    and optionally renders the final state of one specified environment.\n",
    "    Handles hybrid state (grid, coords). Returns key statistics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Evaluation ({num_eval_episodes} episodes) ---\")\n",
    "    eval_start_time = time.time()\n",
    "\n",
    "    eval_env = BatchedSculpt3DEnvTF(grid_size=grid_size, max_steps=max_steps, n_envs=num_eval_episodes)\n",
    "\n",
    "    initial_shape_mask_flat_gpu = eval_env.shape_mask[0]\n",
    "    initial_shape_mask_flat_np = initial_shape_mask_flat_gpu.numpy()\n",
    "    initial_carvable_mask_flat = ~initial_shape_mask_flat_np\n",
    "    initial_carvable_count = np.sum(initial_carvable_mask_flat)\n",
    "    print(f\"  Initial number of carvable voxels: {initial_carvable_count}\")\n",
    "    if initial_carvable_count == 0: print(\"  Warning: No carvable material defined.\")\n",
    "\n",
    "    all_ep_rewards = []\n",
    "    all_ep_lengths = []\n",
    "    all_ep_removed_counts = []\n",
    "    all_ep_incorrect_removed_counts = []\n",
    "\n",
    "    final_stock_variable_eval = eval_env.stock\n",
    "\n",
    "    obs_tuple = eval_env.reset()\n",
    "    done = tf.zeros([num_eval_episodes], tf.bool)\n",
    "    ep_rewards = tf.zeros([num_eval_episodes], tf.float32)\n",
    "    ep_steps = tf.zeros([num_eval_episodes], tf.int32)\n",
    "    current_ep_step = 0\n",
    "\n",
    "    while not tf.reduce_all(done):\n",
    "        if current_ep_step >= max_steps: break\n",
    "        A = agent.act_batch(obs_tuple, deterministic=True) # Use deterministic actions\n",
    "        S2_tuple, R, next_done = eval_env.step(A)\n",
    "        active_mask = ~done\n",
    "        ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "        ep_steps += tf.cast(active_mask, tf.int32)\n",
    "        obs_tuple = S2_tuple\n",
    "        done = next_done\n",
    "        current_ep_step += 1\n",
    "\n",
    "    final_stock_batch_np = final_stock_variable_eval.numpy()\n",
    "    batch_rewards = ep_rewards.numpy()\n",
    "    batch_lengths = ep_steps.numpy()\n",
    "    all_ep_rewards.extend(batch_rewards.tolist())\n",
    "    all_ep_lengths.extend(batch_lengths.tolist())\n",
    "\n",
    "    for i in range(num_eval_episodes):\n",
    "        final_stock_flat_np = final_stock_batch_np[i]\n",
    "        removed_mask = initial_carvable_mask_flat & (~final_stock_flat_np)\n",
    "        removed_count = np.sum(removed_mask)\n",
    "        all_ep_removed_counts.append(removed_count)\n",
    "        incorrectly_removed_mask = initial_shape_mask_flat_np & (~final_stock_flat_np)\n",
    "        incorrectly_removed_count = np.sum(incorrectly_removed_mask)\n",
    "        all_ep_incorrect_removed_counts.append(incorrectly_removed_count)\n",
    "\n",
    "    # Aggregate and Print Statistics\n",
    "    avg_reward = np.mean(all_ep_rewards); std_reward = np.std(all_ep_rewards)\n",
    "    avg_length = np.mean(all_ep_lengths)\n",
    "    avg_removed_count = np.mean(all_ep_removed_counts)\n",
    "    avg_incorrect_removed = np.mean(all_ep_incorrect_removed_counts)\n",
    "    avg_removal_percentage = (avg_removed_count / initial_carvable_count) * 100.0 if initial_carvable_count > 0 else 0.0\n",
    "    std_removal_percentage = np.std([(c / initial_carvable_count)*100.0 if initial_carvable_count > 0 else 0.0 for c in all_ep_removed_counts])\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"  Avg Reward : {avg_reward:.2f} (+/- {std_reward:.2f})\")\n",
    "    print(f\"  Avg Length : {avg_length:.1f}\")\n",
    "    print(f\"  Avg Removed: {avg_removed_count:.1f} / {initial_carvable_count} ({avg_removal_percentage:.2f}% +/- {std_removal_percentage:.2f}%)\")\n",
    "    print(f\"  Avg Incorrect: {avg_incorrect_removed:.1f}\")\n",
    "    eval_duration = time.time() - eval_start_time\n",
    "    print(f\"  Evaluation Duration: {eval_duration:.2f}s\")\n",
    "\n",
    "    # Render Final State\n",
    "    if render:\n",
    "        print(f\"\\n--- Rendering final state for Eval Env Index: {render_env_index} ---\")\n",
    "        if render_env_index < 0 or render_env_index >= num_eval_episodes:\n",
    "             print(f\"Error: render_env_index ({render_env_index}) out of bounds.\")\n",
    "        elif num_eval_episodes > 0:\n",
    "            try:\n",
    "                final_stock_flat_np = final_stock_batch_np[render_env_index]\n",
    "                shape_mask_flat_np = initial_shape_mask_flat_np\n",
    "                G = grid_size\n",
    "                final_stock_3d = final_stock_flat_np.reshape((G, G, G))\n",
    "                shape_mask_3d = shape_mask_flat_np.reshape((G, G, G))\n",
    "                shape_to_plot = shape_mask_3d\n",
    "                initial_carvable_mask_render = ~shape_mask_3d\n",
    "                removed_mask_render = initial_carvable_mask_render & (~final_stock_3d)\n",
    "                incorrectly_removed_mask_render = shape_mask_3d & (~final_stock_3d)\n",
    "\n",
    "                fig = plt.figure(figsize=(9, 7)); ax = fig.add_subplot(111, projection='3d')\n",
    "                ax.set_facecolor('whitesmoke')\n",
    "                x_vox, y_vox, z_vox = np.indices(np.array(shape_to_plot.shape) + 1)\n",
    "                ax.voxels(x_vox, y_vox, z_vox, shape_to_plot, facecolors='blue', alpha=0.1, edgecolor=None)\n",
    "                ax.voxels(x_vox, y_vox, z_vox, removed_mask_render, facecolors='red', alpha=0.6, edgecolor=None)\n",
    "                if np.sum(incorrectly_removed_mask_render) > 0:\n",
    "                     ax.voxels(x_vox, y_vox, z_vox, incorrectly_removed_mask_render, facecolors='yellow', alpha=0.7, edgecolor='orange', label='Incorrect Removal')\n",
    "                     ax.legend()\n",
    "\n",
    "                rendered_env_reward = all_ep_rewards[render_env_index]\n",
    "                rendered_env_perc = (all_ep_removed_counts[render_env_index] / initial_carvable_count) * 100.0 if initial_carvable_count > 0 else 0.0\n",
    "                ax.set_title(f\"Eval Render Env #{render_env_index} (R={rendered_env_reward:.1f}, Removed={rendered_env_perc:.1f}%)\")\n",
    "                ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "                ax.set_xlim(0, G); ax.set_ylim(0, G); ax.set_zlim(0, G); ax.set_aspect('auto')\n",
    "                plt.tight_layout()\n",
    "                render_dir = \"renders_eval\"\n",
    "                if not os.path.exists(render_dir): os.makedirs(render_dir)\n",
    "                save_path = os.path.join(render_dir, f\"eval_render_env_{render_env_index}_final.png\")\n",
    "                plt.savefig(save_path); print(f\"Saved evaluation render to {save_path}\"); plt.close(fig)\n",
    "            except Exception as e: print(f\"Error during rendering: {e}\"); import traceback; traceback.print_exc()\n",
    "\n",
    "    # <<< MODIFIED: Return statistics >>>\n",
    "    return {\n",
    "        \"avg_reward\": avg_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"avg_length\": avg_length,\n",
    "        \"avg_removed_count\": avg_removed_count,\n",
    "        \"avg_incorrect_removed\": avg_incorrect_removed,\n",
    "        \"avg_removal_percentage\": avg_removal_percentage,\n",
    "        \"std_removal_percentage\": std_removal_percentage,\n",
    "        \"initial_carvable_count\": initial_carvable_count\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Training Loop (Using Hybrid State & Noisy Agent)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_gpu_batched(\n",
    "        grid_size=16, max_steps=300, n_envs=32, episodes=10000,\n",
    "        buffer_capacity=100000, learn_batch_size=32, learn_freq=4,\n",
    "        gamma=0.99, lr=1e-4, tau=0.005, log_every=50,\n",
    "        evaluate_every=100,\n",
    "        num_eval_episodes_periodic=10,\n",
    "        render_intermediate_eval=False\n",
    "        ):\n",
    "\n",
    "    print(f\"--- Training Hybrid Noisy DQN Agent ---\")\n",
    "    print(f\"Params: Grid={grid_size}, N_Envs={n_envs}, MaxSteps={max_steps}, Episodes={episodes}\")\n",
    "    print(f\"Learn Batch Size (B): {learn_batch_size}, Total samples/learn step (B*N): {learn_batch_size * n_envs}\")\n",
    "    print(f\"Periodic Evaluation every {evaluate_every} episodes ({num_eval_episodes_periodic} eps each).\")\n",
    "    print(f\"Memory Warning: Ensure sufficient CPU RAM and GPU VRAM.\")\n",
    "\n",
    "    env = BatchedSculpt3DEnvTF(grid_size, max_steps, n_envs)\n",
    "    agent = BatchedDQNAgentTF(grid_shape=env.grid_obs_shape, coord_shape=env.coord_obs_shape,\n",
    "                              action_dim=6, lr=lr, gamma=gamma, tau=tau)\n",
    "    agent.buffer.cap = buffer_capacity\n",
    "\n",
    "    total_steps_taken = 0\n",
    "    episode_rewards_history = []\n",
    "    episode_lengths_history = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # <<< New lists to store evaluation results >>>\n",
    "    eval_episodes_list = []\n",
    "    eval_avg_rewards_list = []\n",
    "    eval_avg_removal_perc_list = []\n",
    "\n",
    "    for ep in range(1, episodes + 1):\n",
    "        obs_tuple = env.reset()\n",
    "        done = tf.zeros([n_envs], tf.bool)\n",
    "        ep_rewards = tf.zeros([n_envs], tf.float32)\n",
    "        ep_steps = tf.zeros([n_envs], tf.int32)\n",
    "        current_ep_step = 0\n",
    "\n",
    "        while not tf.reduce_all(done):\n",
    "            A = agent.act_batch(obs_tuple, deterministic=False)\n",
    "            S2_tuple, R, next_done = env.step(A)\n",
    "            agent.remember_batch(obs_tuple, A, R, S2_tuple, next_done)\n",
    "            active_mask = ~done\n",
    "            ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "            ep_steps += tf.cast(active_mask, tf.int32)\n",
    "            obs_tuple = S2_tuple\n",
    "            done = next_done\n",
    "            total_steps_taken += n_envs\n",
    "            current_ep_step += 1\n",
    "            if current_ep_step > 0 and current_ep_step % learn_freq == 0:\n",
    "                loss_val = agent.learn(learn_batch_size)\n",
    "\n",
    "        avg_reward_batch = tf.reduce_mean(ep_rewards).numpy()\n",
    "        avg_steps_batch = tf.reduce_mean(tf.cast(ep_steps, tf.float32)).numpy()\n",
    "        episode_rewards_history.append(avg_reward_batch)\n",
    "        episode_lengths_history.append(avg_steps_batch)\n",
    "\n",
    "        if ep % log_every == 0 or ep == 1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_r = np.mean(episode_rewards_history[-log_every:]) if episode_rewards_history else 0.0\n",
    "            avg_l = np.mean(episode_lengths_history[-log_every:]) if episode_lengths_history else 0.0\n",
    "            print(f\"Ep {ep}/{episodes} | Avg R: {avg_r:.2f} | Avg Len: {avg_l:.1f} | Steps: {total_steps_taken} | Time: {elapsed_time:.1f}s\")\n",
    "            agent_step = agent.train_step_count.numpy()\n",
    "            with agent.writer.as_default(step=agent_step):\n",
    "                 tf.summary.scalar(\"Episode/AvgReward\", avg_r)\n",
    "                 tf.summary.scalar(\"Episode/AvgLength\", avg_l)\n",
    "\n",
    "        # --- Periodic Evaluation Call & Data Storage ---\n",
    "        if evaluate_every > 0 and ep % evaluate_every == 0:\n",
    "            eval_stats = evaluate_agent_performance(\n",
    "                agent=agent,\n",
    "                grid_size=grid_size,\n",
    "                max_steps=max_steps,\n",
    "                num_eval_episodes=num_eval_episodes_periodic,\n",
    "                render=render_intermediate_eval,\n",
    "                render_env_index=0\n",
    "            )\n",
    "            if eval_stats: # Check if evaluation ran successfully\n",
    "                eval_episodes_list.append(ep)\n",
    "                eval_avg_rewards_list.append(eval_stats[\"avg_reward\"])\n",
    "                eval_avg_removal_perc_list.append(eval_stats[\"avg_removal_percentage\"])\n",
    "                # Log eval stats to TensorBoard too\n",
    "                agent_step = agent.train_step_count.numpy() # Use agent step for consistency\n",
    "                with agent.writer.as_default(step=agent_step):\n",
    "                    tf.summary.scalar(\"Evaluate/AvgReward\", eval_stats[\"avg_reward\"])\n",
    "                    tf.summary.scalar(\"Evaluate/AvgRemovalPercentage\", eval_stats[\"avg_removal_percentage\"])\n",
    "                    tf.summary.scalar(\"Evaluate/AvgIncorrectRemoved\", eval_stats[\"avg_incorrect_removed\"])\n",
    "\n",
    "            print(\"-\" * 60) # Separator after evaluation\n",
    "\n",
    "    # --- End of Training ---\n",
    "    agent.writer.close()\n",
    "    print(f\"Training finished. Total steps: {total_steps_taken}\")\n",
    "\n",
    "    # --- Plot Evaluation Trend ---\n",
    "    if eval_episodes_list:\n",
    "        print(\"\\n--- Plotting Evaluation Trend ---\")\n",
    "        try:\n",
    "            fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            color = 'tab:red'\n",
    "            ax1.set_xlabel('Training Episode')\n",
    "            ax1.set_ylabel('Avg Carvable Material Removed (%)', color=color)\n",
    "            ax1.plot(eval_episodes_list, eval_avg_removal_perc_list, color=color, marker='o', linestyle='-', label='Removal %')\n",
    "            ax1.tick_params(axis='y', labelcolor=color)\n",
    "            ax1.grid(True, axis='y', linestyle=':')\n",
    "\n",
    "            # Optional: Plot average reward on second y-axis\n",
    "            ax2 = ax1.twinx()\n",
    "            color = 'tab:blue'\n",
    "            ax2.set_ylabel('Avg Evaluation Reward', color=color)\n",
    "            ax2.plot(eval_episodes_list, eval_avg_rewards_list, color=color, marker='x', linestyle='--', label='Avg Reward')\n",
    "            ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "            fig.suptitle('Agent Evaluation Performance During Training')\n",
    "            fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "            # Add legend if both axes are plotted\n",
    "            # lines, labels = ax1.get_legend_handles_labels()\n",
    "            # lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "            # ax2.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "\n",
    "            plot_dir = \"plots\"\n",
    "            if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "            plot_path = os.path.join(plot_dir, f\"evaluation_trend_g{grid_size}_n{n_envs}.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"Saved evaluation trend plot to {plot_path}\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting evaluation trend: {e}\")\n",
    "\n",
    "    return trained_agent\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Main Execution Block (Adjusted Parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters (Adjust based on your hardware!) ---\n",
    "    GRID_SIZE_RUN = 8\n",
    "    N_ENVS_RUN = 16\n",
    "    MAX_STEPS_RUN = 500\n",
    "    EPISODES_RUN = 5000\n",
    "    BUFFER_CAP_RUN = 100000\n",
    "    LEARN_BATCH_RUN = 32\n",
    "    LEARNING_RATE = 1e-4\n",
    "    EVAL_FREQ_RUN = 50 # Evaluate every 100 training episodes\n",
    "    NUM_EVAL_EPISODES_RUN = 10 # Run 10 episodes for each evaluation\n",
    "\n",
    "    print(f\"Starting run with Hybrid State + Noisy Nets\")\n",
    "    print(f\"ACTUAL Params: Grid={GRID_SIZE_RUN}, N_Envs={N_ENVS_RUN}, LearnBatch(B)={LEARN_BATCH_RUN}\")\n",
    "    print(f\"Total samples per learn step (B*N): {LEARN_BATCH_RUN * N_ENVS_RUN}\")\n",
    "\n",
    "    # Train the agent, with periodic evaluation\n",
    "    trained_agent = train_gpu_batched(\n",
    "        grid_size=GRID_SIZE_RUN,\n",
    "        max_steps=MAX_STEPS_RUN,\n",
    "        n_envs=N_ENVS_RUN,\n",
    "        episodes=EPISODES_RUN,\n",
    "        buffer_capacity=BUFFER_CAP_RUN,\n",
    "        learn_batch_size=LEARN_BATCH_RUN,\n",
    "        learn_freq=4,\n",
    "        gamma=0.99,\n",
    "        lr=LEARNING_RATE,\n",
    "        tau=0.005,\n",
    "        log_every=50,\n",
    "        evaluate_every=EVAL_FREQ_RUN,\n",
    "        num_eval_episodes_periodic=NUM_EVAL_EPISODES_RUN,\n",
    "        render_intermediate_eval=False # Keep intermediate rendering off by default\n",
    "    )\n",
    "\n",
    "    # --- Run Final Evaluation After Training ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"      RUNNING FINAL EVALUATION ON TRAINED AGENT\")\n",
    "    print(\"=\"*70)\n",
    "    if trained_agent:\n",
    "         evaluate_agent_performance(\n",
    "             agent=trained_agent,\n",
    "             grid_size=GRID_SIZE_RUN,\n",
    "             max_steps=MAX_STEPS_RUN,\n",
    "             num_eval_episodes=100, # Evaluate over more episodes for final assessment\n",
    "             render=True,          # Render the final plot for one episode\n",
    "             render_env_index=0\n",
    "         )\n",
    "\n",
    "    # --- Optional: Save Model Weights ---\n",
    "    # if trained_agent:\n",
    "    #     save_path = f\"sculpt_hybrid_noisy_dqn_g{GRID_SIZE_RUN}_weights.h5\"\n",
    "    #     try:\n",
    "    #         trained_agent.model.save_weights(save_path)\n",
    "    #         print(f\"\\nModel weights saved to {save_path}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"\\nError saving model weights: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% imports (ensure these are imported in your notebook)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Assume BatchedSculpt3DEnvTF and BatchedDQNAgentTF classes are defined\n",
    "# Assume trained_agent, GRID_SIZE_TO_RUN, MAX_STEPS_TRAIN are available\n",
    "# from the previous training cell.\n",
    "\n",
    "# %% Evaluation Function Definition\n",
    "\n",
    "def evaluate_agent_stats(agent, grid_size, max_steps, num_test_episodes=10, render_env_index=0):\n",
    "    \"\"\"\n",
    "    Evaluates a trained agent greedily, calculates performance statistics,\n",
    "    and renders the final state of one specified environment.\n",
    "\n",
    "    Args:\n",
    "        agent: The trained BatchedDQNAgentTF object.\n",
    "        grid_size: The grid size used during training.\n",
    "        max_steps: The max steps per episode used during training.\n",
    "        num_test_episodes: How many episodes to run for evaluation.\n",
    "        render_env_index: The index of the environment (0 to num_test_episodes-1)\n",
    "                           whose final state will be rendered.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Agent Evaluation ---\")\n",
    "    print(f\"Running {num_test_episodes} evaluation episodes...\")\n",
    "\n",
    "    # Create a test environment batch\n",
    "    test_env = BatchedSculpt3DEnvTF(grid_size=grid_size, max_steps=max_steps, n_envs=num_test_episodes)\n",
    "\n",
    "    # --- Get Initial State Information ---\n",
    "    # We need the shape mask to know what *can* be carved.\n",
    "    # Since reset() sets stock to all True, initial carvable material is just ~shape_mask.\n",
    "    initial_shape_mask_flat_gpu = test_env.shape_mask[0] # Mask is same for all envs\n",
    "    initial_shape_mask_flat_np = initial_shape_mask_flat_gpu.numpy()\n",
    "    # Calculate initial carvable material count (where stock is True and mask is False)\n",
    "    initial_carvable_mask_flat = ~initial_shape_mask_flat_np # Initially stock is True everywhere\n",
    "    initial_carvable_count = np.sum(initial_carvable_mask_flat)\n",
    "    print(f\"Initial number of carvable voxels: {initial_carvable_count}\")\n",
    "    if initial_carvable_count == 0:\n",
    "        print(\"Warning: No carvable material initially defined by the shape mask.\")\n",
    "\n",
    "    # --- Run Evaluation Episodes ---\n",
    "    all_ep_rewards = []\n",
    "    all_ep_lengths = []\n",
    "    all_ep_removed_counts = []\n",
    "    all_ep_removal_percentages = []\n",
    "\n",
    "    # References to final state variables\n",
    "    final_stock_variable_test = test_env.stock\n",
    "    final_shape_mask_variable_test = test_env.shape_mask # Although constant, get reference\n",
    "\n",
    "    for ep in range(num_test_episodes):\n",
    "        obs = test_env.reset()\n",
    "        done = tf.zeros([num_test_episodes], tf.bool)\n",
    "        ep_rewards = tf.zeros([num_test_episodes], tf.float32)\n",
    "        ep_steps = tf.zeros([num_test_episodes], tf.int32)\n",
    "        current_ep_step = 0\n",
    "\n",
    "        while not tf.reduce_all(done):\n",
    "            # Act greedily (epsilon = 0)\n",
    "            eps_tf_zero = tf.constant(0.0, dtype=tf.float32)\n",
    "            A = agent.act_batch(obs, eps_tf_zero)\n",
    "            S2, R, next_done = test_env.step(A)\n",
    "\n",
    "            active_mask = ~done\n",
    "            ep_rewards += R * tf.cast(active_mask, tf.float32)\n",
    "            ep_steps += tf.cast(active_mask, tf.int32)\n",
    "\n",
    "            obs = S2\n",
    "            done = next_done\n",
    "            current_ep_step += 1\n",
    "\n",
    "        # --- Calculate Stats for this Batch ---\n",
    "        final_stock_batch_np = final_stock_variable_test.numpy() # Get final stock for all envs in batch\n",
    "        batch_rewards = ep_rewards.numpy()\n",
    "        batch_lengths = ep_steps.numpy()\n",
    "\n",
    "        all_ep_rewards.extend(batch_rewards.tolist())\n",
    "        all_ep_lengths.extend(batch_lengths.tolist())\n",
    "\n",
    "        # Calculate removal stats per environment\n",
    "        for i in range(num_test_episodes):\n",
    "            final_stock_flat_np = final_stock_batch_np[i]\n",
    "            # Material that was initially carvable and is now removed (~final_stock)\n",
    "            removed_mask = initial_carvable_mask_flat & (~final_stock_flat_np)\n",
    "            removed_count = np.sum(removed_mask)\n",
    "            all_ep_removed_counts.append(removed_count)\n",
    "\n",
    "            if initial_carvable_count > 0:\n",
    "                removal_percentage = (removed_count / initial_carvable_count) * 100.0\n",
    "                all_ep_removal_percentages.append(removal_percentage)\n",
    "            else:\n",
    "                all_ep_removal_percentages.append(0.0) # Avoid division by zero\n",
    "\n",
    "        print(f\"Eval Episode {ep+1}/{num_test_episodes} finished. Steps: {current_ep_step}\") # Removed rewards print here\n",
    "\n",
    "    # --- Aggregate and Print Statistics ---\n",
    "    if not all_ep_rewards:\n",
    "        print(\"\\nNo evaluation episodes were run.\")\n",
    "        return\n",
    "\n",
    "    avg_reward = np.mean(all_ep_rewards)\n",
    "    avg_length = np.mean(all_ep_lengths)\n",
    "    avg_removed_count = np.mean(all_ep_removed_counts)\n",
    "    avg_removal_percentage = np.mean(all_ep_removal_percentages)\n",
    "    std_reward = np.std(all_ep_rewards)\n",
    "    std_removal_percentage = np.std(all_ep_removal_percentages)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results ({num_test_episodes} episodes) ---\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f} (+/- {std_reward:.2f})\")\n",
    "    print(f\"Average Length: {avg_length:.1f}\")\n",
    "    print(f\"Average Carvable Voxels Removed: {avg_removed_count:.1f} / {initial_carvable_count}\")\n",
    "    print(f\"Average Removal Percentage: {avg_removal_percentage:.2f}% (+/- {std_removal_percentage:.2f}%)\")\n",
    "\n",
    "    # --- Render Final State ---\n",
    "    print(f\"\\n--- Rendering final state for Eval Env Index: {render_env_index} ---\")\n",
    "    if render_env_index < 0 or render_env_index >= num_test_episodes:\n",
    "         print(f\"Error: render_env_index ({render_env_index}) out of bounds for {num_test_episodes} test environments.\")\n",
    "         return\n",
    "\n",
    "    try:\n",
    "        # Get final stock and shape mask for the specific env index from the *last* batch run\n",
    "        final_stock_flat_np = final_stock_batch_np[render_env_index]\n",
    "        # Mask is constant, can re-use from initial calculation\n",
    "        # final_shape_mask_flat_np = final_shape_mask_variable_test[0].numpy()\n",
    "\n",
    "        # Reshape to 3D grid\n",
    "        G = grid_size\n",
    "        final_stock_3d = final_stock_flat_np.reshape((G, G, G))\n",
    "        final_shape_mask_3d = initial_shape_mask_flat_np.reshape((G, G, G))\n",
    "\n",
    "        # Create boolean arrays for plotting:\n",
    "        shape_to_plot = final_shape_mask_3d\n",
    "        removed_material_to_plot = ~final_stock_3d & ~final_shape_mask_3d\n",
    "\n",
    "        # --- Matplotlib Visualization ---\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        x, y, z = np.indices(np.array(shape_to_plot.shape) + 1) # Edges\n",
    "\n",
    "        # Plot the target shape (slightly transparent blue)\n",
    "        ax.voxels(x, y, z, shape_to_plot, facecolors='blue', alpha=0.15, edgecolor=None)\n",
    "        # Plot the removed material (more opaque red)\n",
    "        ax.voxels(x, y, z, removed_material_to_plot, facecolors='red', alpha=0.5, edgecolor=None)\n",
    "\n",
    "        # Use the reward from the specific rendered env\n",
    "        rendered_env_reward = all_ep_rewards[render_env_index]\n",
    "        rendered_env_perc = all_ep_removal_percentages[render_env_index]\n",
    "        title_reward = f\"{rendered_env_reward:.2f}\"\n",
    "        ax.set_title(f\"Eval Render Env #{render_env_index} (R={title_reward}, Removed={rendered_env_perc:.1f}%)\")\n",
    "\n",
    "        ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "        ax.set_xlim(0, G); ax.set_ylim(0, G); ax.set_zlim(0, G)\n",
    "        ax.set_aspect('auto')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save figure\n",
    "        render_dir = \"renders_eval\"\n",
    "        if not os.path.exists(render_dir):\n",
    "            os.makedirs(render_dir)\n",
    "        save_path = os.path.join(render_dir, f\"eval_render_env_{render_env_index}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved evaluation render to {save_path}\")\n",
    "        plt.close(fig) # Close figure\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during rendering: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# %% Run Evaluation (Example Call)\n",
    "# Make sure trained_agent, GRID_SIZE_TO_RUN, and MAX_STEPS_TRAIN are defined\n",
    "# from your training process before running this cell.\n",
    "\n",
    "NUM_EVAL_EPISODES = 10 # Number of episodes to average over for evaluation\n",
    "RENDER_INDEX = 0       # Which episode's final state to render (0 to NUM_EVAL_EPISODES-1)\n",
    "\n",
    "if 'trained_agent' in locals() or 'trained_agent' in globals():\n",
    "     evaluate_agent_stats(\n",
    "         agent=trained_agent,\n",
    "         grid_size=GRID_SIZE_TO_RUN,\n",
    "         max_steps=MAX_STEPS_TRAIN,\n",
    "         num_test_episodes=NUM_EVAL_EPISODES,\n",
    "         render_env_index=RENDER_INDEX\n",
    "     )\n",
    "else:\n",
    "     print(\"Variable 'trained_agent' not found. Please train the agent first.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Install widgets if needed (uncomment and run once)\n",
    "# !pip install ipywidgets\n",
    "\n",
    "# %% Imports and Setup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Assume BatchedSculpt3DEnvTF and BatchedDQNAgentTF classes are defined above\n",
    "# Assume trained_agent, GRID_SIZE_TO_RUN, MAX_STEPS_TRAIN are available\n",
    "\n",
    "print(\"Imports and setup complete.\")\n",
    "\n",
    "# %% --- Helper Functions for Predefined Shapes ---\n",
    "# These functions create the 1D flat boolean mask for different shapes\n",
    "\n",
    "def create_sphere_mask(grid_size):\n",
    "    \"\"\"Generates a flat boolean mask for a centered sphere.\"\"\"\n",
    "    G = grid_size\n",
    "    coords_range = tf.range(G, dtype=tf.float32)\n",
    "    coords = tf.stack(tf.meshgrid(\n",
    "        coords_range, coords_range, coords_range, indexing='ij'\n",
    "    ), axis=-1)\n",
    "    center = tf.constant([G/2 - 0.5, G/2 - 0.5, G/2 - 0.5], tf.float32)\n",
    "    dist2 = tf.reduce_sum(tf.square(coords - center), axis=-1)\n",
    "    radius_sq = tf.square(tf.cast(G // 2 - 1, tf.float32)) # Sphere radius G/2 - 1\n",
    "    mask3d = dist2 <= radius_sq\n",
    "    mask_flat = tf.reshape(mask3d, [-1])\n",
    "    return mask_flat.numpy() # Return as numpy array\n",
    "\n",
    "def create_cube_mask(grid_size, cube_ratio=0.5):\n",
    "    \"\"\"Generates a flat boolean mask for a centered cube.\"\"\"\n",
    "    G = grid_size\n",
    "    cube_size = int(G * cube_ratio)\n",
    "    start = (G - cube_size) // 2\n",
    "    end = start + cube_size\n",
    "    mask3d = np.zeros((G, G, G), dtype=bool)\n",
    "    mask3d[start:end, start:end, start:end] = True\n",
    "    mask_flat = mask3d.reshape([-1])\n",
    "    return mask_flat\n",
    "\n",
    "# Add more shape functions here if desired (e.g., cylinder, multiple objects)\n",
    "\n",
    "# %% --- Simulation, Statistics, and Plotting Function ---\n",
    "\n",
    "def run_simulation_and_display(selected_shape_name, grid_size, max_steps, agent):\n",
    "    \"\"\"\n",
    "    Runs the agent on a selected shape, calculates stats, and displays results.\n",
    "    \"\"\"\n",
    "    global output_widget # Use the globally defined output widget\n",
    "\n",
    "    # --- 1. Clear Output & Generate Shape Mask ---\n",
    "    with output_widget:\n",
    "        clear_output(wait=True) # Clear previous results\n",
    "        print(f\"Selected Target Shape: {selected_shape_name}\")\n",
    "        print(f\"Grid Size: {grid_size}x{grid_size}x{grid_size}, Max Steps: {max_steps}\")\n",
    "        print(\"Generating shape mask...\")\n",
    "        time.sleep(0.1) # Tiny pause for UI update\n",
    "\n",
    "        if selected_shape_name == 'Sphere':\n",
    "            shape_mask_flat = create_sphere_mask(grid_size)\n",
    "        elif selected_shape_name == 'Cube':\n",
    "            shape_mask_flat = create_cube_mask(grid_size, cube_ratio=0.6) # Example: 60% cube\n",
    "        # Add elif for other shapes here\n",
    "        else:\n",
    "             print(f\"Error: Shape generation function for '{selected_shape_name}' not found.\")\n",
    "             return\n",
    "\n",
    "        try:\n",
    "            shape_mask_3d = shape_mask_flat.reshape((grid_size, grid_size, grid_size))\n",
    "            print(\"Shape mask generated.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error reshaping mask: {e}\")\n",
    "            return\n",
    "\n",
    "    # --- 2. Setup Environment (n_envs=1) ---\n",
    "    # Create a new environment instance for this specific run\n",
    "    # This ensures state isolation but might incur tf.function retracing on first run per shape\n",
    "    try:\n",
    "        # print(\"DEBUG: Creating test environment...\") # Temporary debug\n",
    "        test_env = BatchedSculpt3DEnvTF(grid_size=grid_size, max_steps=max_steps, n_envs=1)\n",
    "        # Set the custom shape mask *after* initialization\n",
    "        # This assumes the shape_mask variable exists and can be assigned to.\n",
    "        test_env.shape_mask.assign(tf.tile(tf.constant(shape_mask_flat, dtype=tf.bool)[None,:], [1, 1]))\n",
    "        # print(\"DEBUG: Test environment created and mask assigned.\") # Temporary debug\n",
    "    except Exception as e:\n",
    "         with output_widget:\n",
    "              print(f\"Error creating environment: {e}\")\n",
    "              import traceback\n",
    "              traceback.print_exc()\n",
    "         return\n",
    "\n",
    "    # --- 3. Run Greedy Episode & Track Path ---\n",
    "    with output_widget:\n",
    "        print(\"Running agent simulation (greedy)...\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    sim_start_time = time.time()\n",
    "    obs = test_env.reset()\n",
    "    done = tf.zeros([1], tf.bool) # Batch size is 1\n",
    "    path_coords = []\n",
    "    total_reward = 0.0\n",
    "    steps_taken = 0\n",
    "    eps_tf_zero = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "    # Store initial position for path plotting\n",
    "    try:\n",
    "        initial_pos_flat = test_env.pos.numpy()[0]\n",
    "        x_i = initial_pos_flat // (grid_size*grid_size)\n",
    "        y_i = (initial_pos_flat // grid_size) % grid_size\n",
    "        z_i = initial_pos_flat % grid_size\n",
    "        path_coords.append([x_i, y_i, z_i])\n",
    "    except Exception as e:\n",
    "        with output_widget:\n",
    "            print(f\"Error getting initial position: {e}\")\n",
    "        # Continue without initial point if needed\n",
    "\n",
    "    # Simulation loop\n",
    "    while not tf.reduce_all(done):\n",
    "        if steps_taken >= max_steps: # Safety break\n",
    "             print(f\"Warning: Reached max_steps ({max_steps}) during simulation loop.\")\n",
    "             break\n",
    "        try:\n",
    "            A = agent.act_batch(obs, eps_tf_zero)\n",
    "            S2, R, next_done = test_env.step(A)\n",
    "\n",
    "            # Get current position for path tracking (from observation S2)\n",
    "            current_pos_xyz = S2.numpy()[0, :3] # Get numpy array for the single env [x,y,z]\n",
    "            path_coords.append(current_pos_xyz.tolist())\n",
    "\n",
    "            total_reward += R.numpy()[0] # Get reward for the single env\n",
    "            steps_taken += 1\n",
    "            obs = S2\n",
    "            done = next_done\n",
    "\n",
    "        except Exception as e:\n",
    "            with output_widget:\n",
    "                print(f\"\\nError during simulation step {steps_taken}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            return # Stop simulation on error\n",
    "\n",
    "    sim_duration = time.time() - sim_start_time\n",
    "    with output_widget:\n",
    "        print(f\"Simulation finished in {sim_duration:.2f} seconds.\")\n",
    "\n",
    "    # --- 4. Get Final State ---\n",
    "    try:\n",
    "        final_stock_flat = test_env.stock.numpy()[0] # Get stock for the single env\n",
    "        final_stock_3d = final_stock_flat.reshape((grid_size, grid_size, grid_size))\n",
    "    except Exception as e:\n",
    "         with output_widget:\n",
    "              print(f\"Error getting final stock state: {e}\")\n",
    "         return\n",
    "\n",
    "    # --- 5. Calculate Statistics ---\n",
    "    initial_carvable_mask = ~shape_mask_3d # Stock is initially True everywhere outside mask\n",
    "    initial_carvable_count = np.sum(initial_carvable_mask)\n",
    "\n",
    "    removed_mask = initial_carvable_mask & (~final_stock_3d) # Initially carvable AND now removed\n",
    "    removed_count = np.sum(removed_mask)\n",
    "\n",
    "    incorrectly_removed_mask = shape_mask_3d & (~final_stock_3d) # Target shape AND now removed (error)\n",
    "    incorrectly_removed_count = np.sum(incorrectly_removed_mask)\n",
    "\n",
    "    removal_percentage = (removed_count / initial_carvable_count) * 100.0 if initial_carvable_count > 0 else 0.0\n",
    "\n",
    "    # --- 6. Plotting ---\n",
    "    with output_widget:\n",
    "        print(\"\\n--- Simulation Results ---\")\n",
    "        print(f\"  Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"  Steps Taken: {steps_taken}\")\n",
    "        print(f\"  Carvable Voxels Removed: {removed_count} / {initial_carvable_count} ({removal_percentage:.1f}%)\")\n",
    "        print(f\"  Target Voxels Incorrectly Removed: {incorrectly_removed_count}\")\n",
    "        print(\"\\nPlotting final state and path...\")\n",
    "\n",
    "        try:\n",
    "            fig = plt.figure(figsize=(10, 8)) # Slightly wider figure\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.set_facecolor('whitesmoke') # Nicer background\n",
    "\n",
    "            # Voxel coordinates (for edges)\n",
    "            x_vox, y_vox, z_vox = np.indices(np.array(shape_mask_3d.shape) + 1)\n",
    "\n",
    "            # Plot target shape (transparent blue) - Plotting only surface might be faster\n",
    "            ax.voxels(x_vox, y_vox, z_vox, shape_mask_3d, facecolors='blue', alpha=0.1, edgecolor=None)\n",
    "\n",
    "            # Plot removed material (red) - Only where originally carvable\n",
    "            ax.voxels(x_vox, y_vox, z_vox, removed_mask, facecolors='red', alpha=0.6, edgecolor=None)\n",
    "\n",
    "            # Plot incorrectly removed material (target shape voxels removed - yellow/warning)\n",
    "            if incorrectly_removed_count > 0:\n",
    "                 ax.voxels(x_vox, y_vox, z_vox, incorrectly_removed_mask, facecolors='yellow', alpha=0.7, edgecolor='orange')\n",
    "\n",
    "\n",
    "            # Plot path\n",
    "            if path_coords and len(path_coords) > 1:\n",
    "                path_np = np.array(path_coords)\n",
    "                # Offset path slightly for visibility if needed, or keep as is\n",
    "                ax.plot(path_np[:, 0]+0.5, path_np[:, 1]+0.5, path_np[:, 2]+0.5, color='green', linewidth=1.5, label='Agent Path', alpha=0.8)\n",
    "                # Start/End markers\n",
    "                ax.scatter(path_np[0, 0]+0.5, path_np[0, 1]+0.5, path_np[0, 2]+0.5, color='lime', s=80, edgecolor='black', depthshade=False, label='Start', alpha=1.0)\n",
    "                ax.scatter(path_np[-1, 0]+0.5, path_np[-1, 1]+0.5, path_np[-1, 2]+0.5, color='magenta', s=80, edgecolor='black', depthshade=False, label='End', alpha=1.0)\n",
    "                ax.legend()\n",
    "            elif path_coords: # Only start point\n",
    "                 ax.scatter(path_coords[0][0]+0.5, path_coords[0][1]+0.5, path_coords[0][2]+0.5, color='lime', s=80, edgecolor='black', depthshade=False, label='Start', alpha=1.0)\n",
    "                 ax.legend()\n",
    "\n",
    "\n",
    "            ax.set_title(f\"Agent Result: '{selected_shape_name}' | R={total_reward:.1f} | Removed={removal_percentage:.1f}%\")\n",
    "            ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\n",
    "            ax.set_xlim(0, grid_size); ax.set_ylim(0, grid_size); ax.set_zlim(0, grid_size)\n",
    "            # Setting aspect ratio can be tricky in 3D, 'auto' is often best default\n",
    "            ax.set_aspect('auto') # Try 'equal' if axes look distorted, but might fail\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show() # Display plot directly in Colab output cell\n",
    "            print(\"Plotting complete.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during plotting: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# %% --- Create and Display Widgets ---\n",
    "\n",
    "# Check if agent variable exists (replace 'trained_agent' if yours has a different name)\n",
    "agent_variable_name = 'trained_agent' # <<< MAKE SURE THIS MATCHES YOUR TRAINED AGENT VARIABLE\n",
    "if agent_variable_name not in locals() and agent_variable_name not in globals():\n",
    "     print(f\"ERROR: Trained agent variable '{agent_variable_name}' not found.\")\n",
    "     print(\"Please ensure the agent is trained and the variable is available before running this cell.\")\n",
    "     # You might want to stop execution here in a real notebook\n",
    "     # raise NameError(f\"Variable '{agent_variable_name}' not defined.\")\n",
    "else:\n",
    "    print(\"Trained agent found.\")\n",
    "    # Define Widgets\n",
    "    shape_selector = widgets.Dropdown(\n",
    "        options=['Sphere', 'Cube'], # Add more names if functions are defined above\n",
    "        value='Sphere',\n",
    "        description='Target Shape:',\n",
    "        style={'description_width': 'initial'}, # Prevent label truncation\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    run_button = widgets.Button(\n",
    "        description='Run Simulation & Render',\n",
    "        disabled=False,\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Run the trained agent on the selected shape and display results',\n",
    "        icon='cube' # Example icon\n",
    "    )\n",
    "\n",
    "    # Output widget to capture prints and plots\n",
    "    output_widget = widgets.Output()\n",
    "\n",
    "    # Define button click handler\n",
    "    def on_run_button_clicked(button_instance):\n",
    "        # Disable button during run\n",
    "        run_button.disabled = True\n",
    "        run_button.icon = 'spinner'\n",
    "        shape_name = shape_selector.value\n",
    "        agent_obj = globals().get(agent_variable_name) or locals().get(agent_variable_name)\n",
    "\n",
    "        if agent_obj is None:\n",
    "             with output_widget:\n",
    "                  clear_output(wait=True)\n",
    "                  print(f\"Error: '{agent_variable_name}' not found.\")\n",
    "             run_button.disabled = False # Re-enable button\n",
    "             run_button.icon = 'cube'\n",
    "             return\n",
    "\n",
    "        # Run the simulation function\n",
    "        run_simulation_and_display(\n",
    "            selected_shape_name=shape_name,\n",
    "            grid_size=GRID_SIZE_TO_RUN,     # Assumes this exists from training\n",
    "            max_steps=MAX_STEPS_TRAIN,      # Assumes this exists from training\n",
    "            agent=agent_obj\n",
    "        )\n",
    "        # Re-enable button after run\n",
    "        run_button.disabled = False\n",
    "        run_button.icon = 'cube'\n",
    "\n",
    "\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "\n",
    "    # Display the UI Layout\n",
    "    print(\"\\n--- Agent Evaluation Interface ---\")\n",
    "    ui_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Select a target shape and run the simulation:</h3>\"),\n",
    "        shape_selector,\n",
    "        run_button,\n",
    "        widgets.HTML(\"<hr><h4>Results:</h4>\"), # Separator\n",
    "        output_widget\n",
    "    ])\n",
    "    display(ui_box)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
